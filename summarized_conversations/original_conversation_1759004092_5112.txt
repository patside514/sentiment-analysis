{'role': 'assistant', 'content': '<function_calls>\n<create-file file_path="src/nlp/keyword_extractor.py">\n"""\nKeyword extraction module using multiple techniques.\nSupports TF-IDF, TextRank, and frequency-based extraction.\n"""\nfrom typing import List, Dict, Any, Optional, Tuple\nimport re\nfrom collections import Counter\nfrom itertools import combinations\n\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ntry:\n    import spacy\n    SPACY_AVAILABLE = True\nexcept ImportError:\n    SPACY_AVAILABLE = False\n\nfrom .text_preprocessor import TextPreprocessor\nfrom ..utils.logger import get_logger\n\nlogger = get_logger(__name__)\n\nclass KeywordExtractor:\n    """Multi-technique keyword extractor"""\n    \n    def __init__(self, language: str = \'auto\', max_keywords: int = 50):\n        self.language = language\n        self.max_keywords = max_keywords\n        self.preprocessor = TextPreprocessor(language)\n        self.tfidf_vectorizer = None\n    \n    def extract_keywords(self, texts: List[str], method: str = \'combined\') -> List[Dict[str, Any]]:\n        """Extract keywords from texts using specified method"""\n        if not texts:\n            return []\n        \n        try:\n            if method == \'tfidf\':\n                return self._extract_with_tfidf(texts)\n            elif method == \'frequency\':\n                return self._extract_with_frequency(texts)\n            elif method == \'textrank\':\n                return self._extract_with_textrank(texts)\n            elif method == \'combined\':\n                return self._extract_combined(texts)\n            else:\n                raise ValueError(f"Unknown method: {method}")\n                \n        except Exception as e:\n            logger.error(f"Keyword extraction error: {e}")\n            return []\n    \n    def _extract_with_tfidf(self, texts: List[str]) -> List[Dict[str, Any]]:\n        """Extract keywords using TF-IDF"""\n        try:\n            # Preprocess texts\n            processed_texts = []\n            for text in texts:\n                result = self.preprocessor.preprocess_text(text)\n                processed_texts.append(\' \'.join(result[\'tokens\']))\n            \n            # Create TF-IDF vectorizer\n            self.tfidf_vectorizer = TfidfVectorizer(\n                max_features=1000,\n                ngram_range=(1, 3),\n                min_df=2,\n                max_df=0.8,\n                stop_words=\'english\' if self.language == \'english\' else \'french\'\n            )\n            \n            # Fit and transform\n            tfidf_matrix = self.tfidf_vectorizer.fit_transform(processed_texts)\n            feature_names = self.tfidf_vectorizer.get_feature_names_out()\n            \n            # Calculate mean TF-IDF scores across all documents\n            mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)\n            \n            # Get top keywords\n            top_indices = np.argsort(mean_scores)[-self.max_keywords:][::-1]\n            \n            keywords = []\n            for idx in top_indices:\n                keyword = feature_names[idx]\n                score = mean_scores[idx]\n                \n                if score > 0:\n                    keywords.append({\n                        \'keyword\': keyword,\n                        \'score\': float(score),\n                        \'frequency\': self._calculate_frequency(keyword, processed_texts),\n                        \'method\': \'tfidf\'\n                    })\n            \n            return keywords\n            \n        except Exception as e:\n            logger.error(f"TF-IDF extraction error: {e}")\n            return []\n    \n    def _extract_with_frequency(self, texts: List[str]) -> List[Dict[str, Any]]:\n        """Extract keywords based on frequency"""\n        try:\n            # Preprocess all texts\n            all_tokens = []\n            for text in texts:\n                result = self.preprocessor.preprocess_text(text)\n                all_tokens.extend(result[\'tokens\'])\n            \n            # Count token frequencies\n            token_counts = Counter(all_tokens)\n            \n            # Filter by minimum frequency\n            min_frequency = max(2, len(texts) * 0.01)\n            frequent_tokens = {\n                token: count for token, count in token_counts.items()\n                if count >= min_frequency\n            }\n            \n            # Calculate scores (normalized frequency)\n            max_count = max(frequent_tokens.values()) if frequent_tokens else 1\n            keywords = []\n            \n            for token, count in sorted(frequent_tokens.items(), key=lambda x: x[1], reverse=True):\n                if len(keywords) >= self.max_keywords:\n                    break\n                \n                keywords.append({\n                    \'keyword\': token,\n                    \'score\': count / max_count,\n                    \'frequency\': count,\n                    \'method\': \'frequency\'\n                })\n            \n            return keywords\n            \n        except Exception as e:\n            logger.error(f"Frequency extraction error: {e}")\n            return []\n    \n    def _extract_with_textrank(self, texts: List[str]) -> List[Dict[str, Any]]:\n        """Extract keywords using TextRank algorithm"""\n        if not SPACY_AVAILABLE:\n            logger.warning("spaCy not available, falling back to TF-IDF")\n            return self._extract_with_tfidf(texts)\n        \n        try:\n            # Load spaCy model\n            nlp = spacy.load(\'fr_core_news_sm\' if self.language == \'french\' else \'en_core_web_sm\')\n            \n            # Process texts\n            all_words = []\n            sentences = []\n            \n            for text in texts:\n                doc = nlp(text)\n                sentences.extend(list(doc.sents))\n                \n                for token in doc:\n                    if self._is_valid_token(token):\n                        all_words.append(token.lemma_.lower())\n            \n            # Build word co-occurrence graph\n            co_occurrence = self._build_cooccurrence_matrix(all_words)\n            \n            # Calculate TextRank scores\n            textrank_scores = self._calculate_textrank(co_occurrence)\n            \n            # Get top keywords\n            top_words = sorted(textrank_scores.items(), key=lambda x: x[1], reverse=True)\n            \n            keywords = []\n            for word, score in top_words[:self.max_keywords]:\n                frequency = all_words.count(word)\n                keywords.append({\n                    \'keyword\': word,\n                    \'score\': score,\n                    \'frequency\': frequency,\n                    \'method\': \'textrank\'\n                })\n            \n            return keywords\n            \n        except Exception as e:\n            logger.error(f"TextRank extraction error: {e}")\n            return self._extract_with_tfidf(texts)  # Fallback\n    \n    def _extract_combined(self, texts: List[str]) -> List[Dict[str, Any]]:\n        """Combine multiple keyword extraction methods"""\n        # Extract keywords using different methods\n        tfidf_keywords = {k[\'keyword\']: k for k in self._extract_with_tfidf(texts)}\n        frequency_keywords = {k[\'keyword\']: k for k in self._extract_with_frequency(texts)}\n        textrank_keywords = {k[\'keyword\']: k for k in self._extract_with_textrank(texts)}\n        \n        # Combine all unique keywords\n        all_keywords = set(tfidf_keywords.keys()) | set(frequency_keywords.keys()) | set(textrank_keywords.keys())\n        \n        combined_keywords = []\n        for keyword in all_keywords:\n            scores = []\n            frequencies = []\n            methods = []\n            \n            if keyword in tfidf_keywords:\n                scores.append(tfidf_keywords[keyword][\'score\'])\n                frequencies.append(tfidf_keywords[keyword][\'frequency\'])\n                methods.append(\'tfidf\')\n            \n            if keyword in frequency_keywords:\n                scores.append(frequency_keywords[keyword][\'score\'])\n                frequencies.append(frequency_keywords[keyword][\'frequency\'])\n                methods.append(\'frequency\')\n            \n            if keyword in textrank_keywords:\n                scores.append(textrank_keywords[keyword][\'score\'])\n                frequencies.append(textrank_keywords[keyword][\'frequency\'])\n                methods.append(\'textrank\')\n            \n            # Calculate combined score (average)\n            combined_score = np.mean(scores)\n            total_frequency = sum(frequencies)\n            \n            combined_keywords.append({\n                \'keyword\': keyword,\n                \'score\': combined_score,\n                \'frequency\': total_frequency,\n                \'methods\': methods,\n                \'method\': \'combined\'\n            })\n        \n        # Sort by combined score\n        combined_keywords.sort(key=lambda x: x[\'score\'], reverse=True)\n        \n        return combined_keywords[:self.max_keywords]\n    \n    def _build_cooccurrence_matrix(self, words: List[str], window_size: int = 4) -> Dict[str, Dict[str, int]]:\n        """Build co-occurrence matrix for TextRank"""\n        co_occurrence = {}\n        \n        for i, word in enumerate(words):\n            if word not in co_occurrence:\n                co_occurrence[word] = {}\n            \n            # Look at words within window\n            start = max(0, i - window_size)\n            end = min(len(words), i + window_size + 1)\n            \n            for j in range(start, end):\n                if i != j:\n                    other_word = words[j]\n                    if other_word not in co_occurrence[word]:\n                        co_occurrence[word][other_word] = 0\n                    co_occurrence[word][other_word] += 1\n        \n        return co_occurrence\n    \n    def _calculate_textrank(self, co_occurrence: Dict[str, Dict[str, int]], \n                           max_iterations: int = 100, damping_factor: float = 0.85) -> Dict[str, float]:\n        """Calculate TextRank scores"""\n        words = list(co_occurrence.keys())\n        scores = {word: 1.0 for word in words}\n        \n        for _ in range(max_iterations):\n            new_scores = {}\n            \n            for word in words:\n                score = (1 - damping_factor)\n                \n                # Add contribution from neighbors\n                for other_word, weight in co_occurrence[word].items():\n                    if other_word in scores:\n                        other_total_weight = sum(co_occurrence[other_word].values())\n                        if other_total_weight > 0:\n                            score += damping_factor * (scores[other_word] * weight / other_total_weight)\n                \n                new_scores[word] = score\n            \n            scores = new_scores\n        \n        return scores\n    \n    def _is_valid_token(self, token) -> bool:\n        """Check if spaCy token is valid for keyword extraction"""\n        return (token.is_alpha and \n                not token.is_stop and \n                not token.is_punct and \n                len(token.text) > 2)\n    \n    def _calculate_frequency(self, keyword: str, texts: List[str]) -> int:\n        """Calculate frequency of keyword in texts"""\n        count = 0\n        for text in texts:\n            count += text.lower().count(keyword.lower())\n        return count\n    \n    def extract_key_phrases(self, texts: List[str], min_length: int = 2, \n                          max_length: int = 4) -> List[Dict[str, Any]]:\n        """Extract key phrases (multi-word keywords)"""\n        try:\n            # Preprocess texts\n            processed_texts = []\n            for text in texts:\n                result = self.preprocessor.preprocess_text(text)\n                processed_texts.append(\' \'.join(result[\'tokens\']))\n            \n            # Extract n-grams\n            all_ngrams = []\n            for n in range(min_length, max_length + 1):\n                ngrams = self._extract_ngrams(processed_texts, n)\n                all_ngrams.extend(ngrams)\n            \n            # Count n-gram frequencies\n            ngram_counts = Counter(all_ngrams)\n            \n            # Filter by minimum frequency\n            min_frequency = max(2, len(texts) * 0.005)\n            frequent_ngrams = {\n                ngram: count for ngram, count in ngram_counts.items()\n                if count >= min_frequency\n            }\n            \n            # Calculate scores\n            phrases = []\n            for ngram, count in sorted(frequent_ngrams.items(), key=lambda x: x[1], reverse=True):\n                if len(phrases) >= self.max_keywords:\n                    break\n                \n                phrases.append({\n                    \'keyword\': \' \'.join(ngram),\n                    \'score\': count / max(frequent_ngrams.values()),\n                    \'frequency\': count,\n                    \'length\': len(ngram),\n                    \'method\': \'ngram\'\n                })\n            \n            return phrases\n            \n        except Exception as e:\n            logger.error(f"Key phrase extraction error: {e}")\n            return []\n    \n    def _extract_ngrams(self, texts: List[str], n: int) -> List[tuple]:\n        """Extract n-grams from texts"""\n        ngrams = []\n        \n        for text in texts:\n            words = text.split()\n            if len(words) >= n:\n                for i in range(len(words) - n + 1):\n                    ngram = tuple(words[i:i + n])\n                    ngrams.append(ngram)\n        \n        return ngrams\n    \n    def get_keyword_trends(self, keywords: List[str], texts: List[str], \n                          dates: List[str]) -> Dict[str, Any]:\n        """Analyze keyword trends over time"""\n        if len(texts) != len(dates):\n            raise ValueError("Texts and dates lists must have the same length")\n        \n        trends = {}\n        \n        for keyword in keywords:\n            keyword_trend = []\n            \n            for text, date in zip(texts, dates):\n                # Count keyword occurrences\n                count = text.lower().count(keyword.lower())\n                \n                keyword_trend.append({\n                    \'date\': date,\n                    \'count\': count,\n                    \'present\': count > 0\n                })\n            \n            trends[keyword] = keyword_trend\n        \n        return trends\n</create-file>\n</function_calls>'}