{'role': 'assistant', 'content': '<function_calls>\n<create-file file_path="src/extractors/google_reviews_extractor.py">\n"""\nGoogle Reviews data extractor using Google Places API and web scraping.\nHandles business reviews and ratings extraction.\n"""\nimport re\nimport time\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict, Any, Optional\nimport requests\nfrom urllib.parse import urlencode, quote_plus\n\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\n\nfrom .base_extractor import BaseExtractor, ExtractionError, RateLimitError, AuthenticationError\nfrom ..config import APIConfig\nfrom ..utils.logger import get_logger\n\nlogger = get_logger(__name__)\n\nclass GoogleReviewsExtractor(BaseExtractor):\n    """Google Reviews data extractor"""\n    \n    def __init__(self, service: str, max_posts: int = 500):\n        super().__init__(service, max_posts)\n        self.api_key = APIConfig.GOOGLE_API_KEY\n        self.base_url = "https://maps.googleapis.com/maps/api"\n        self._validate_credentials()\n    \n    def _validate_credentials(self):\n        """Validate Google API credentials"""\n        if not self.api_key:\n            logger.warning("Google API key not configured, will use web scraping only")\n        else:\n            try:\n                # Test API connection\n                test_url = f"{self.base_url}/place/textsearch/json"\n                params = {\n                    \'query\': \'test\',\n                    \'key\': self.api_key\n                }\n                response = requests.get(test_url, params=params)\n                \n                if response.status_code == 200:\n                    logger.info("Google API authentication successful")\n                else:\n                    logger.warning(f"Google API test failed: {response.text}")\n                    \n            except Exception as e:\n                logger.error(f"Google API validation failed: {e}")\n    \n    def extract_posts(self, days: int = 30, **kwargs) -> List[Dict[str, Any]]:\n        """Extract Google reviews"""\n        logger.info(f"Extracting Google reviews for \'{self.service}\' from last {days} days")\n        \n        try:\n            # Search for businesses\n            businesses = self._search_businesses(self.service)\n            \n            if not businesses:\n                logger.warning(f"No businesses found for service: {self.service}")\n                return []\n            \n            all_reviews = []\n            for business in businesses[:5]:  # Limit to top 5 businesses\n                business_reviews = self._extract_business_reviews(business[\'place_id\'], days)\n                all_reviews.extend(business_reviews)\n                \n                if len(all_reviews) >= self.max_posts:\n                    break\n            \n            # Limit to max_posts\n            reviews = all_reviews[:self.max_posts]\n            self.posts_extracted = len(reviews)\n            \n            logger.info(f"Extracted {len(reviews)} Google reviews")\n            return reviews\n            \n        except Exception as e:\n            self._handle_extraction_error(e, "Google reviews extraction")\n            return []\n    \n    def _search_businesses(self, query: str) -> List[Dict[str, Any]]:\n        """Search for businesses using Google Places API"""\n        businesses = []\n        \n        if not self.api_key:\n            # Fallback to web scraping\n            return self._search_businesses_scrape(query)\n        \n        try:\n            search_url = f"{self.base_url}/place/textsearch/json"\n            params = {\n                \'query\': query,\n                \'key\': self.api_key,\n                \'language\': \'fr\',\n                \'region\': \'fr\'\n            }\n            \n            response = requests.get(search_url, params=params)\n            \n            if response.status_code == 200:\n                data = response.json()\n                \n                if data.get(\'status\') == \'OK\':\n                    businesses = data.get(\'results\', [])\n                    \n                    # Sort by rating and user ratings count\n                    businesses.sort(\n                        key=lambda x: (x.get(\'rating\', 0), x.get(\'user_ratings_total\', 0)), \n                        reverse=True\n                    )\n                    \n                    logger.info(f"Found {len(businesses)} businesses for query: {query}")\n                else:\n                    logger.warning(f"Google Places API error: {data.get(\'status\')}")\n                    \n        except Exception as e:\n            logger.error(f"Error searching businesses: {e}")\n            # Fallback to scraping\n            businesses = self._search_businesses_scrape(query)\n        \n        return businesses\n    \n    def _search_businesses_scrape(self, query: str) -> List[Dict[str, Any]]:\n        """Fallback business search using web scraping"""\n        businesses = []\n        \n        try:\n            # Construct Google search URL\n            search_query = f"{query} avis google"\n            encoded_query = quote_plus(search_query)\n            search_url = f"https://www.google.com/search?q={encoded_query}"\n            \n            headers = {\n                \'User-Agent\': \'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\'\n            }\n            \n            response = requests.get(search_url, headers=headers)\n            \n            if response.status_code == 200:\n                soup = BeautifulSoup(response.content, \'html.parser\')\n                \n                # Extract business information from search results\n                # This is a simplified extraction - real implementation would need\n                # more sophisticated parsing based on Google\'s HTML structure\n                business_elements = soup.find_all(\'div\', class_=\'BNeawe\')\n                \n                for element in business_elements[:10]:  # Limit results\n                    business_name = element.get_text(strip=True)\n                    if business_name and len(business_name) > 3:\n                        businesses.append({\n                            \'name\': business_name,\n                            \'place_id\': f"scrape_{len(businesses)}",\n                            \'rating\': 0,\n                            \'user_ratings_total\': 0,\n                            \'formatted_address\': \'Unknown\',\n                            \'types\': [\'local_business\']\n                        })\n                \n                logger.info(f"Scraped {len(businesses)} businesses")\n            \n        except Exception as e:\n            logger.error(f"Error scraping businesses: {e}")\n        \n        return businesses\n    \n    def _extract_business_reviews(self, place_id: str, days: int) -> List[Dict[str, Any]]:\n        """Extract reviews for a specific business"""\n        reviews = []\n        start_date, end_date = self._calculate_date_range(days)\n        \n        try:\n            if self.api_key and not place_id.startswith(\'scrape_\'):\n                # Use Google Places API\n                reviews = self._extract_reviews_api(place_id, start_date, end_date)\n            else:\n                # Use web scraping\n                reviews = self._extract_reviews_scrape(place_id, start_date, end_date)\n            \n        except Exception as e:\n            logger.error(f"Error extracting reviews for place {place_id}: {e}")\n        \n        return reviews\n    \n    def _extract_reviews_api(self, place_id: str, start_date: datetime, \n                           end_date: datetime) -> List[Dict[str, Any]]:\n        """Extract reviews using Google Places API"""\n        reviews = []\n        \n        try:\n            # First, get place details\n            details_url = f"{self.base_url}/place/details/json"\n            params = {\n                \'place_id\': place_id,\n                \'fields\': \'name,rating,user_ratings_total,review\',\n                \'key\': self.api_key,\n                \'language\': \'fr\'\n            }\n            \n            response = requests.get(details_url, params=params)\n            \n            if response.status_code == 200:\n                data = response.json()\n                \n                if data.get(\'status\') == \'OK\':\n                    place_details = data.get(\'result\', {})\n                    \n                    # Extract reviews\n                    api_reviews = place_details.get(\'reviews\', [])\n                    \n                    for review in api_reviews:\n                        review_date = datetime.fromtimestamp(review[\'time\'])\n                        \n                        # Check if review is within date range\n                        if start_date <= review_date <= end_date:\n                            processed_review = self._process_api_review(review, place_details)\n                            if processed_review:\n                                reviews.append(processed_review)\n                                self.posts_extracted += 1\n                    \n                    logger.info(f"Extracted {len(reviews)} reviews via API for {place_details.get(\'name\', \'Unknown\')}")\n                \n        except Exception as e:\n            logger.error(f"Google Places API error: {e}")\n        \n        return reviews\n    \n    def _extract_reviews_scrape(self, place_id: str, start_date: datetime, \n                              end_date: datetime) -> List[Dict[str, Any]]:\n        """Extract reviews using web scraping"""\n        reviews = []\n        \n        try:\n            # Construct Google Maps URL for the business\n            # This is a simplified approach - real implementation would need\n            # to handle the specific business URL\n            maps_url = f"https://www.google.com/maps/place/{quote_plus(self.service)}"\n            \n            headers = {\n                \'User-Agent\': \'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\'\n            }\n            \n            response = requests.get(maps_url, headers=headers)\n            \n            if response.status_code == 200:\n                soup = BeautifulSoup(response.content, \'html.parser\')\n                \n                # Extract review elements\n                # Note: Google\'s HTML structure changes frequently\n                review_elements = soup.find_all(\'div\', {\'data-review-id\': True})\n                \n                for element in review_elements[:20]:  # Limit reviews per business\n                    review = self._parse_review_element(element)\n                    if review:\n                        review_date = datetime.fromisoformat(review[\'created_at\'].replace(\'Z\', \'+00:00\'))\n                        \n                        if start_date <= review_date <= end_date:\n                            reviews.append(review)\n                            self.posts_extracted += 1\n                \n                logger.info(f"Scraped {len(reviews)} reviews for {self.service}")\n            \n        except Exception as e:\n            logger.error(f"Error scraping reviews: {e}")\n        \n        return reviews\n    \n    def _process_api_review(self, review: Dict[str, Any], \n                          place_details: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n        """Process Google API review"""\n        try:\n            review_data = {\n                \'review_id\': review.get(\'review_id\', review.get(\'time\', \'\')),\n                \'text\': review.get(\'text\', \'\'),\n                \'rating\': review.get(\'rating\', 0),\n                \'created_at\': datetime.fromtimestamp(review[\'time\']).isoformat(),\n                \'author_name\': review.get(\'author_name\', \'Anonymous\'),\n                \'author_url\': review.get(\'author_url\', \'\'),\n                \'profile_photo_url\': review.get(\'profile_photo_url\', \'\'),\n                \'business_name\': place_details.get(\'name\', \'Unknown\'),\n                \'business_rating\': place_details.get(\'rating\', 0),\n                \'total_reviews\': place_details.get(\'user_ratings_total\', 0),\n                \'type\': \'google_review\'\n            }\n            \n            # Validate and clean\n            validated_review = self._validate_and_clean_post(review_data)\n            if validated_review:\n                return self._normalize_post_structure(validated_review, \'google_reviews\')\n            \n        except Exception as e:\n            logger.error(f"Error processing Google API review: {e}")\n            self.errors_count += 1\n        \n        return None\n    \n    def _parse_review_element(self, element) -> Optional[Dict[str, Any]]:\n        """Parse review element from scraped HTML"""\n        try:\n            # Extract review text\n            text_element = element.find(\'span\', class_=\'wiI7pd\')\n            text = text_element.get_text(strip=True) if text_element else \'\'\n            \n            if not text:\n                return None\n            \n            # Extract rating\n            rating_element = element.find(\'span\', class_=\'kvMYJc\')\n            rating = 0\n            if rating_element:\n                # Count star elements\n                stars = rating_element.find_all(\'img\')\n                rating = len([s for s in stars if \'star\' in s.get(\'src\', \'\')])\n            \n            # Extract author\n            author_element = element.find(\'div\', class_=\'d4r55\')\n            author_name = author_element.get_text(strip=True) if author_element else \'Anonymous\'\n            \n            # Extract date\n            date_element = element.find(\'span\', class_=\'rsqaWe\')\n            date_text = date_element.get_text(strip=True) if date_element else \'\'\n            review_date = self._parse_review_date(date_text)\n            \n            review_data = {\n                \'review_id\': element.get(\'data-review-id\', \'\'),\n                \'text\': text,\n                \'rating\': rating,\n                \'created_at\': review_date.isoformat(),\n                \'author_name\': author_name,\n                \'business_name\': self.service,\n                \'type\': \'google_review\'\n            }\n            \n            # Validate and clean\n            validated_review = self._validate_and_clean_post(review_data)\n            if validated_review:\n                return self._normalize_post_structure(validated_review, \'google_reviews\')\n            \n        except Exception as e:\n            logger.error(f"Error parsing review element: {e}")\n            self.errors_count += 1\n        \n        return None\n    \n    def _parse_review_date(self, date_text: str) -> datetime:\n        """Parse review date from text"""\n        try:\n            # Handle relative dates like "il y a 2 jours", "2 weeks ago"\n            if \'jour\' in date_text.lower() or \'day\' in date_text.lower():\n                days = int(re.search(r\'\\d+\', date_text).group() or 0)\n                return datetime.now() - timedelta(days=days)\n            elif \'semaine\' in date_text.lower() or \'week\' in date_text.lower():\n                weeks = int(re.search(r\'\\d+\', date_text).group() or 0)\n                return datetime.now() - timedelta(weeks=weeks)\n            elif \'mois\' in date_text.lower() or \'month\' in date_text.lower():\n                months = int(re.search(r\'\\d+\', date_text).group() or 0)\n                return datetime.now() - timedelta(days=months * 30)\n            else:\n                # Try to parse as date\n                return datetime.now() - timedelta(days=30)  # Default fallback\n                \n        except Exception:\n            return datetime.now() - timedelta(days=30)\n    \n    def search_posts(self, query: str, max_results: int = 100) -> List[Dict[str, Any]]:\n        """Search for Google reviews"""\n        # For Google reviews, we search for businesses and get their reviews\n        businesses = self._search_businesses(query)\n        reviews = []\n        \n        for business in businesses[:3]:  # Limit to top 3 businesses\n            business_reviews = self._extract_business_reviews(business[\'place_id\'], 30)\n            \n            # Filter reviews containing the query\n            for review in business_reviews:\n                if query.lower() in review.get(\'text\', \'\').lower():\n                    reviews.append(review)\n                \n                if len(reviews) >= max_results:\n                    break\n            \n            if len(reviews) >= max_results:\n                break\n        \n        return reviews[:max_results]\n    \n    def get_post_by_id(self, post_id: str) -> Optional[Dict[str, Any]]:\n        """Get Google review by ID"""\n        # Google reviews don\'t have a simple ID-based retrieval\n        # This would require knowing the business and then searching through reviews\n        logger.warning("Google review retrieval by ID not implemented")\n        return None\n</create-file>\n</function_calls>'}