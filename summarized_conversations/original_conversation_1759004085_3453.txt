{'role': 'assistant', 'content': '<function_calls>\n<create-file file_path="src/extractors/twitter_extractor.py">\n"""\nTwitter data extractor using Tweepy and snscrape as fallback.\nHandles Twitter API v2 and legacy API access.\n"""\nimport re\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict, Any, Optional\nimport time\n\nimport pandas as pd\nfrom tqdm import tqdm\n\ntry:\n    import tweepy\n    TWEEPY_AVAILABLE = True\nexcept ImportError:\n    TWEEPY_AVAILABLE = False\n\ntry:\n    import snscrape.modules.twitter as sntwitter\n    SN_SCRAPE_AVAILABLE = True\nexcept ImportError:\n    SN_SCRAPE_AVAILABLE = False\n\nfrom .base_extractor import BaseExtractor, ExtractionError, RateLimitError, AuthenticationError\nfrom ..config import APIConfig\nfrom ..utils.logger import get_logger\n\nlogger = get_logger(__name__)\n\nclass TwitterExtractor(BaseExtractor):\n    """Twitter data extractor"""\n    \n    def __init__(self, service: str, max_posts: int = 500):\n        super().__init__(service, max_posts)\n        self.api = None\n        self.client = None\n        self._setup_api()\n    \n    def _setup_api(self):\n        """Setup Twitter API connections"""\n        try:\n            if TWEEPY_AVAILABLE and APIConfig.TWITTER_BEARER_TOKEN:\n                # Twitter API v2\n                self.client = tweepy.Client(\n                    bearer_token=APIConfig.TWITTER_BEARER_TOKEN,\n                    wait_on_rate_limit=True\n                )\n                \n                # Legacy API for additional functionality\n                if (APIConfig.TWITTER_API_KEY and \n                    APIConfig.TWITTER_API_SECRET and\n                    APIConfig.TWITTER_ACCESS_TOKEN and\n                    APIConfig.TWITTER_ACCESS_TOKEN_SECRET):\n                    \n                    auth = tweepy.OAuthHandler(\n                        APIConfig.TWITTER_API_KEY,\n                        APIConfig.TWITTER_API_SECRET\n                    )\n                    auth.set_access_token(\n                        APIConfig.TWITTER_ACCESS_TOKEN,\n                        APIConfig.TWITTER_ACCESS_TOKEN_SECRET\n                    )\n                    self.api = tweepy.API(auth, wait_on_rate_limit=True)\n                \n                logger.info("Twitter API setup successful")\n            else:\n                logger.warning("Twitter API credentials not available, will use snscrape")\n                \n        except Exception as e:\n            logger.error(f"Twitter API setup failed: {e}")\n            raise AuthenticationError(f"Twitter API authentication failed: {e}")\n    \n    def extract_posts(self, days: int = 30, **kwargs) -> List[Dict[str, Any]]:\n        """Extract tweets using API or snscrape"""\n        logger.info(f"Extracting Twitter posts for \'{self.service}\' from last {days} days")\n        \n        try:\n            if self.client:\n                return self._extract_with_api(days, **kwargs)\n            elif SN_SCRAPE_AVAILABLE:\n                return self._extract_with_snscrape(days, **kwargs)\n            else:\n                raise ExtractionError("No Twitter extraction method available")\n                \n        except Exception as e:\n            self._handle_extraction_error(e, "Twitter extraction")\n            return []\n    \n    def _extract_with_api(self, days: int, **kwargs) -> List[Dict[str, Any]]:\n        """Extract tweets using Twitter API v2"""\n        posts = []\n        start_date, end_date = self._calculate_date_range(days)\n        \n        try:\n            # Build query\n            query = self._build_search_query()\n            \n            # Search recent tweets\n            tweets = tweepy.Paginator(\n                self.client.search_recent_tweets,\n                query=query,\n                max_results=100,  # Max per request\n                start_time=start_date.isoformat() + \'Z\',\n                end_time=end_date.isoformat() + \'Z\',\n                tweet_fields=[\'created_at\', \'author_id\', \'public_metrics\', \'lang\'],\n                user_fields=[\'username\', \'name\'],\n                expansions=[\'author_id\']\n            )\n            \n            # Process tweets\n            for tweet_batch in tweets:\n                if len(posts) >= self.max_posts:\n                    break\n                \n                for tweet in tweet_batch.data:\n                    if len(posts) >= self.max_posts:\n                        break\n                    \n                    processed_tweet = self._process_tweet(tweet)\n                    if processed_tweet:\n                        posts.append(processed_tweet)\n                        self.posts_extracted += 1\n                \n                # Rate limiting\n                self._rate_limit_delay(0.5)\n                \n        except tweepy.TooManyRequests as e:\n            logger.warning(f"Twitter rate limit reached: {e}")\n            raise RateLimitError("Twitter rate limit exceeded")\n        except Exception as e:\n            logger.error(f"Twitter API error: {e}")\n            raise ExtractionError(f"Twitter API error: {e}")\n        \n        logger.info(f"Extracted {len(posts)} tweets via API")\n        return posts\n    \n    def _extract_with_snscrape(self, days: int, **kwargs) -> List[Dict[str, Any]]:\n        """Extract tweets using snscrape as fallback"""\n        posts = []\n        start_date, end_date = self._calculate_date_range(days)\n        \n        try:\n            query = self._build_search_query()\n            \n            # Build snscrape query with date range\n            since_date = start_date.strftime("%Y-%m-%d")\n            until_date = end_date.strftime("%Y-%m-%d")\n            snscrape_query = f"{query} since:{since_date} until:{until_date}"\n            \n            logger.info(f"Using snscrape with query: {snscrape_query}")\n            \n            # Search tweets\n            for i, tweet in enumerate(\n                sntwitter.TwitterSearchScraper(snscrape_query).get_items()\n            ):\n                if len(posts) >= self.max_posts:\n                    break\n                \n                processed_tweet = self._process_snscrape_tweet(tweet)\n                if processed_tweet:\n                    posts.append(processed_tweet)\n                    self.posts_extracted += 1\n                \n                # Progress tracking\n                if i % 50 == 0:\n                    logger.info(f"Processed {i} tweets, extracted {len(posts)}")\n            \n        except Exception as e:\n            logger.error(f"snscrape error: {e}")\n            raise ExtractionError(f"snscrape extraction failed: {e}")\n        \n        logger.info(f"Extracted {len(posts)} tweets via snscrape")\n        return posts\n    \n    def _build_search_query(self) -> str:\n        """Build Twitter search query"""\n        service_terms = self.service.lower().split()\n        \n        # Create query with service name and variations\n        query_parts = []\n        for term in service_terms:\n            query_parts.append(term)\n            query_parts.append(f"#{term}")\n            query_parts.append(f"@{term}")\n        \n        # Add language filter (optional)\n        query = " OR ".join(query_parts)\n        \n        # Exclude retweets for original content\n        query += " -is:retweet"\n        \n        return query\n    \n    def _process_tweet(self, tweet) -> Optional[Dict[str, Any]]:\n        """Process Twitter API tweet object"""\n        try:\n            tweet_data = {\n                \'id\': str(tweet.id),\n                \'text\': tweet.text,\n                \'created_at\': tweet.created_at.isoformat(),\n                \'author_id\': str(tweet.author_id),\n                \'lang\': getattr(tweet, \'lang\', \'unknown\'),\n                \'likes\': tweet.public_metrics.get(\'like_count\', 0),\n                \'retweets\': tweet.public_metrics.get(\'retweet_count\', 0),\n                \'replies\': tweet.public_metrics.get(\'reply_count\', 0),\n                \'quotes\': tweet.public_metrics.get(\'quote_count\', 0)\n            }\n            \n            # Validate and clean\n            validated_tweet = self._validate_and_clean_post(tweet_data)\n            if validated_tweet:\n                return self._normalize_post_structure(validated_tweet, \'twitter\')\n            \n        except Exception as e:\n            logger.error(f"Error processing tweet {getattr(tweet, \'id\', \'unknown\')}: {e}")\n            self.errors_count += 1\n        \n        return None\n    \n    def _process_snscrape_tweet(self, tweet) -> Optional[Dict[str, Any]]:\n        """Process snscrape tweet object"""\n        try:\n            tweet_data = {\n                \'id\': str(tweet.id),\n                \'text\': tweet.content,\n                \'created_at\': tweet.date.isoformat(),\n                \'username\': tweet.user.username,\n                \'display_name\': tweet.user.displayname,\n                \'likes\': tweet.likeCount,\n                \'retweets\': tweet.retweetCount,\n                \'replies\': tweet.replyCount,\n                \'quotes\': tweet.quoteCount,\n                \'lang\': getattr(tweet, \'lang\', \'unknown\')\n            }\n            \n            # Validate and clean\n            validated_tweet = self._validate_and_clean_post(tweet_data)\n            if validated_tweet:\n                return self._normalize_post_structure(validated_tweet, \'twitter\')\n            \n        except Exception as e:\n            logger.error(f"Error processing snscrape tweet: {e}")\n            self.errors_count += 1\n        \n        return None\n    \n    def search_posts(self, query: str, max_results: int = 100) -> List[Dict[str, Any]]:\n        """Search for specific tweets"""\n        posts = []\n        \n        try:\n            if self.client:\n                tweets = self.client.search_recent_tweets(\n                    query=query,\n                    max_results=min(max_results, 100),\n                    tweet_fields=[\'created_at\', \'author_id\', \'public_metrics\']\n                )\n                \n                for tweet in tweets.data:\n                    processed_tweet = self._process_tweet(tweet)\n                    if processed_tweet:\n                        posts.append(processed_tweet)\n                        self.posts_extracted += 1\n            \n            elif SN_SCRAPE_AVAILABLE:\n                # Use snscrape for search\n                for i, tweet in enumerate(\n                    sntwitter.TwitterSearchScraper(query).get_items()\n                ):\n                    if len(posts) >= max_results:\n                        break\n                    \n                    processed_tweet = self._process_snscrape_tweet(tweet)\n                    if processed_tweet:\n                        posts.append(processed_tweet)\n                        self.posts_extracted += 1\n            \n        except Exception as e:\n            logger.error(f"Twitter search error: {e}")\n            self._handle_extraction_error(e, "Twitter search")\n        \n        return posts\n    \n    def get_post_by_id(self, post_id: str) -> Optional[Dict[str, Any]]:\n        """Get tweet by ID"""\n        try:\n            if self.client:\n                tweet = self.client.get_tweet(\n                    post_id,\n                    tweet_fields=[\'created_at\', \'author_id\', \'public_metrics\']\n                )\n                \n                if tweet.data:\n                    return self._process_tweet(tweet.data)\n            \n            elif SN_SCRAPE_AVAILABLE:\n                # Use snscrape to get tweet by ID\n                for tweet in sntwitter.TwitterTweetScraper(post_id).get_items():\n                    return self._process_snscrape_tweet(tweet)\n            \n        except Exception as e:\n            logger.error(f"Error getting tweet {post_id}: {e}")\n            self._handle_extraction_error(e, f"Get tweet {post_id}")\n        \n        return None\n</create-file>\n</function_calls>'}