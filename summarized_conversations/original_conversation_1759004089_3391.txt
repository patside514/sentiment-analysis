{'role': 'assistant', 'content': 'Maintenant, créons les modules NLP pour l\'analyse de sentiment et l\'extraction de mots-clés.\n\n<function_calls>\n<create-file file_path="src/nlp/text_preprocessor.py">\n"""\nText preprocessing utilities for NLP analysis.\nHandles cleaning, normalization, and language detection.\n"""\nimport re\nimport string\nfrom typing import List, Dict, Any, Optional\nimport unicodedata\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import SnowballStemmer\n\ntry:\n    import spacy\n    SPACY_AVAILABLE = True\nexcept ImportError:\n    SPACY_AVAILABLE = False\n\nfrom ..utils.logger import get_logger\n\nlogger = get_logger(__name__)\n\n# Download required NLTK data\ntry:\n    nltk.download(\'punkt\', quiet=True)\n    nltk.download(\'stopwords\', quiet=True)\n    nltk.download(\'punkt_tab\', quiet=True)\nexcept Exception as e:\n    logger.warning(f"Could not download NLTK data: {e}")\n\nclass TextPreprocessor:\n    """Text preprocessing utilities"""\n    \n    def __init__(self, language: str = \'auto\'):\n        self.language = language\n        self.stop_words = {}\n        self.stemmers = {}\n        self._load_language_resources()\n    \n    def _load_language_resources(self):\n        """Load language-specific resources"""\n        languages = [\'french\', \'english\']\n        \n        for lang in languages:\n            try:\n                # Load stopwords\n                self.stop_words[lang] = set(stopwords.words(lang))\n                \n                # Load stemmer\n                lang_code = \'french\' if lang == \'french\' else \'english\'\n                self.stemmers[lang] = SnowballStemmer(lang_code)\n                \n            except Exception as e:\n                logger.warning(f"Could not load resources for {lang}: {e}")\n    \n    def preprocess_text(self, text: str, language: Optional[str] = None) -> Dict[str, Any]:\n        """Complete text preprocessing pipeline"""\n        if not text or not isinstance(text, str):\n            return {\n                \'original\': text or \'\',\n                \'cleaned\': \'\',\n                \'tokens\': [],\n                \'language\': language or self.language,\n                \'preprocessing_steps\': []\n            }\n        \n        result = {\n            \'original\': text,\n            \'cleaned\': \'\',\n            \'tokens\': [],\n            \'language\': language or self.language,\n            \'preprocessing_steps\': []\n        }\n        \n        try:\n            # Step 1: Basic cleaning\n            cleaned = self._basic_cleaning(text)\n            result[\'preprocessing_steps\'].append(\'basic_cleaning\')\n            \n            # Step 2: Language detection (if auto)\n            if result[\'language\'] == \'auto\':\n                result[\'language\'] = self._detect_language(cleaned)\n                result[\'preprocessing_steps\'].append(\'language_detection\')\n            \n            # Step 3: Advanced cleaning\n            cleaned = self._advanced_cleaning(cleaned, result[\'language\'])\n            result[\'preprocessing_steps\'].append(\'advanced_cleaning\')\n            \n            # Step 4: Tokenization\n            tokens = self._tokenize(cleaned, result[\'language\'])\n            result[\'preprocessing_steps\'].append(\'tokenization\')\n            \n            # Step 5: Remove stopwords\n            tokens = self._remove_stopwords(tokens, result[\'language\'])\n            result[\'preprocessing_steps\'].append(\'stopword_removal\')\n            \n            # Step 6: Stemming (optional)\n            tokens = self._stem_tokens(tokens, result[\'language\'])\n            result[\'preprocessing_steps\'].append(\'stemming\')\n            \n            result[\'cleaned\'] = cleaned\n            result[\'tokens\'] = tokens\n            \n        except Exception as e:\n            logger.error(f"Error in text preprocessing: {e}")\n            result[\'cleaned\'] = text\n            result[\'tokens\'] = text.split()\n        \n        return result\n    \n    def _basic_cleaning(self, text: str) -> str:\n        """Basic text cleaning"""\n        # Remove URLs\n        text = re.sub(r\'http\\S+|www.\\S+\', \'\', text)\n        \n        # Remove email addresses\n        text = re.sub(r\'\\S+@\\S+\', \'\', text)\n        \n        # Remove phone numbers\n        text = re.sub(r\'[\\+\\(]?[0-9][0-9\\s\\-()\\.]{7,}[0-9]\', \'\', text)\n        \n        # Remove extra whitespace\n        text = re.sub(r\'\\s+\', \' \', text)\n        \n        # Remove special characters but keep basic punctuation\n        text = re.sub(r\'[^\\w\\s@#.,!?\\-\\\'"]\', \'\', text)\n        \n        # Normalize unicode\n        text = unicodedata.normalize(\'NFKD\', text)\n        \n        return text.strip().lower()\n    \n    def _advanced_cleaning(self, text: str, language: str) -> str:\n        """Advanced text cleaning based on language"""\n        # Remove extra punctuation\n        text = re.sub(r\'[^\\w\\s]\', \' \', text)\n        \n        # Remove numbers (optional, can be configured)\n        text = re.sub(r\'\\d+\', \'\', text)\n        \n        # Remove single characters\n        text = re.sub(r\'\\b\\w\\b\', \'\', text)\n        \n        # Remove extra whitespace\n        text = re.sub(r\'\\s+\', \' \', text)\n        \n        # Language-specific cleaning\n        if language == \'french\':\n            text = self._french_cleaning(text)\n        elif language == \'english\':\n            text = self._english_cleaning(text)\n        \n        return text.strip()\n    \n    def _french_cleaning(self, text: str) -> str:\n        """French-specific text cleaning"""\n        # Remove common French contractions\n        contractions = {\n            "j\'": "je ", "l\'": "le ", "d\'": "de ", "c\'": "ce ",\n            "n\'": "ne ", "s\'": "se ", "t\'": "te ", "qu\'": "que ",\n            "m\'": "me ", "v\'": "vous "\n        }\n        \n        for contraction, expansion in contractions.items():\n            text = text.replace(contraction, expansion)\n        \n        return text\n    \n    def _english_cleaning(self, text: str) -> str:\n        """English-specific text cleaning"""\n        # Remove common English contractions\n        contractions = {\n            "don\'t": "do not", "won\'t": "will not", "can\'t": "cannot",\n            "n\'t": " not", "\'re": " are", "\'ve": " have", "\'ll": " will",\n            "\'d": " would", "\'m": " am"\n        }\n        \n        for contraction, expansion in contractions.items():\n            text = text.replace(contraction, expansion)\n        \n        return text\n    \n    def _tokenize(self, text: str, language: str) -> List[str]:\n        """Tokenize text"""\n        try:\n            # Use NLTK tokenizer\n            tokens = word_tokenize(text, language=\'french\' if language == \'french\' else \'english\')\n            \n            # Filter tokens\n            tokens = [token for token in tokens if len(token) > 2]\n            tokens = [token for token in tokens if token.isalpha()]\n            \n            return tokens\n            \n        except Exception as e:\n            logger.error(f"Tokenization error: {e}")\n            return text.split()\n    \n    def _remove_stopwords(self, tokens: List[str], language: str) -> List[str]:\n        """Remove stopwords"""\n        stopwords_set = self.stop_words.get(language, set())\n        \n        if not stopwords_set and SPACY_AVAILABLE:\n            # Fallback to spaCy stopwords\n            try:\n                nlp = spacy.load(\'fr_core_news_sm\' if language == \'french\' else \'en_core_web_sm\')\n                stopwords_set = nlp.Defaults.stop_words\n            except Exception:\n                pass\n        \n        return [token for token in tokens if token.lower() not in stopwords_set]\n    \n    def _stem_tokens(self, tokens: List[str], language: str) -> List[str]:\n        """Apply stemming to tokens"""\n        stemmer = self.stemmers.get(language)\n        \n        if stemmer:\n            return [stemmer.stem(token) for token in tokens]\n        \n        return tokens\n    \n    def _detect_language(self, text: str) -> str:\n        """Simple language detection"""\n        # Simple heuristic based on common words\n        french_words = [\'le\', \'la\', \'les\', \'un\', \'une\', \'de\', \'du\', \'des\', \'et\', \'est\', \'sont\']\n        english_words = [\'the\', \'and\', \'is\', \'are\', \'in\', \'on\', \'at\', \'to\', \'for\', \'of\']\n        \n        text_lower = text.lower()\n        \n        french_score = sum(1 for word in french_words if word in text_lower)\n        english_score = sum(1 for word in english_words if word in text_lower)\n        \n        if french_score > english_score:\n            return \'french\'\n        else:\n            return \'english\'\n    \n    def clean_for_sentiment(self, text: str) -> str:\n        """Clean text specifically for sentiment analysis"""\n        if not text or not isinstance(text, str):\n            return ""\n        \n        # Preserve emoticons and basic punctuation for sentiment\n        text = re.sub(r\'http\\S+|www.\\S+\', \'\', text)  # Remove URLs\n        text = re.sub(r\'\\s+\', \' \', text)  # Normalize whitespace\n        text = re.sub(r\'[^\\w\\s@#.,!?\\-\\\'"():;]\', \'\', text)  # Keep sentiment punctuation\n        \n        return text.strip()\n    \n    def extract_mentions_hashtags(self, text: str) -> Dict[str, List[str]]:\n        """Extract mentions and hashtags"""\n        mentions = re.findall(r\'@\\w+\', text)\n        hashtags = re.findall(r\'#\\w+\', text)\n        \n        return {\n            \'mentions\': mentions,\n            \'hashtags\': hashtags\n        }\n</create-file>\n</function_calls>'}