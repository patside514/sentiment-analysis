{'role': 'assistant', 'content': '<function_calls>\n<create-file file_path="src/main.py">\n"""\nMain orchestration module for the social media sentiment analysis application.\nCoordinates all components and manages the analysis workflow.\n"""\nimport time\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Optional, Callable\n\nfrom .config import AnalysisConfig, AVAILABLE_SOURCES\nfrom .extractors.base_extractor import BaseExtractor, ExtractionError\nfrom .extractors.twitter_extractor import TwitterExtractor\nfrom .extractors.facebook_extractor import FacebookExtractor\nfrom .extractors.google_reviews_extractor import GoogleReviewsExtractor\nfrom .nlp.sentiment_analyzer import SentimentAnalyzer, SentimentTrendAnalyzer\nfrom .nlp.keyword_extractor import KeywordExtractor\nfrom .nlp.text_preprocessor import TextPreprocessor\nfrom .visualization.charts_generator import ChartsGenerator\nfrom .visualization.wordcloud_generator import WordCloudGenerator\nfrom .visualization.report_generator import ReportGenerator\nfrom .utils.logger import get_logger, app_logger\nfrom .utils.file_manager import FileManager\nfrom .utils.data_validator import DataValidator\n\nlogger = get_logger(__name__)\n\nclass SocialMediaAnalyzer:\n    """Main orchestrator for social media sentiment analysis"""\n    \n    def __init__(self):\n        self.extractor = None\n        self.sentiment_analyzer = None\n        self.keyword_extractor = None\n        self.text_preprocessor = TextPreprocessor()\n        self.charts_generator = ChartsGenerator()\n        self.wordcloud_generator = WordCloudGenerator()\n        self.report_generator = ReportGenerator()\n        self.file_manager = FileManager()\n        self.validator = DataValidator()\n        \n        logger.info("SocialMediaAnalyzer initialized")\n    \n    def analyze(self, service: str, source: str, days: int = 30, max_posts: int = 500,\n                language: str = \'auto\', sentiment_model: str = \'auto\',\n                keyword_method: str = \'combined\', progress_callback: Optional[Callable] = None) -> Dict[str, Any]:\n        """\n        Perform complete social media sentiment analysis\n        \n        Args:\n            service: Service/brand name to analyze\n            source: Social media source (twitter, facebook, google_reviews)\n            days: Number of days to analyze\n            max_posts: Maximum number of posts to extract\n            language: Language for analysis\n            sentiment_model: Sentiment analysis model to use\n            keyword_method: Keyword extraction method\n            progress_callback: Callback function for progress updates\n        \n        Returns:\n            Complete analysis results dictionary\n        """\n        try:\n            start_time = time.time()\n            \n            if progress_callback:\n                progress_callback(f"Starting analysis for \'{service}\' from {source}")\n            \n            logger.info(f"Starting analysis: service={service}, source={source}, days={days}, max_posts={max_posts}")\n            \n            # Step 1: Data Extraction\n            if progress_callback:\n                progress_callback("Extracting data from social media...")\n            \n            raw_data = self._extract_data(service, source, days, max_posts)\n            \n            if not raw_data:\n                logger.error("No data extracted")\n                return {\'error\': \'No data extracted\', \'success\': False}\n            \n            if progress_callback:\n                progress_callback(f"Extracted {len(raw_data)} posts")\n            \n            # Step 2: Text Preprocessing\n            if progress_callback:\n                progress_callback("Preprocessing text data...")\n            \n            processed_data = self._preprocess_data(raw_data, language)\n            \n            # Step 3: Sentiment Analysis\n            if progress_callback:\n                progress_callback("Analyzing sentiment...")\n            \n            sentiment_results = self._analyze_sentiment(processed_data, sentiment_model, language)\n            \n            # Step 4: Keyword Extraction\n            if progress_callback:\n                progress_callback("Extracting keywords...")\n            \n            keywords = self._extract_keywords(processed_data, keyword_method)\n            \n            # Step 5: Temporal Analysis (if date data available)\n            temporal_analysis = None\n            if any(\'created_at\' in item for item in processed_data):\n                if progress_callback:\n                    progress_callback("Analyzing temporal trends...")\n                temporal_analysis = self._analyze_temporal_trends(sentiment_results, processed_data)\n            \n            # Step 6: Generate Summary Statistics\n            if progress_callback:\n                progress_callback("Generating summary statistics...")\n            \n            summary_stats = self._generate_summary_statistics(\n                sentiment_results, keywords, raw_data, processed_data\n            )\n            \n            # Step 7: Compile Results\n            if progress_callback:\n                progress_callback("Compiling final results...")\n            \n            results = self._compile_results(\n                service, source, days, max_posts, raw_data, processed_data,\n                sentiment_results, keywords, temporal_analysis, summary_stats\n            )\n            \n            # Step 8: Generate Visualizations\n            if progress_callback:\n                progress_callback("Generating visualizations...")\n            \n            self._generate_visualizations(results)\n            \n            # Step 9: Save Results\n            if progress_callback:\n                progress_callback("Saving results...")\n            \n            output_dir = self.file_manager.save_analysis_report(results, service, source)\n            \n            execution_time = time.time() - start_time\n            \n            results[\'metadata\'][\'execution_time\'] = execution_time\n            results[\'metadata\'][\'output_directory\'] = str(output_dir)\n            \n            logger.info(f"Analysis completed successfully in {execution_time:.2f} seconds")\n            logger.info(f"Results saved to: {output_dir}")\n            \n            if progress_callback:\n                progress_callback(f"Analysis completed in {execution_time:.2f} seconds")\n            \n            return results\n            \n        except Exception as e:\n            logger.error(f"Analysis failed: {e}")\n            return {\'error\': str(e), \'success\': False}\n    \n    def _extract_data(self, service: str, source: str, days: int, max_posts: int) -> List[Dict[str, Any]]:\n        """Extract data from social media source"""\n        try:\n            # Initialize appropriate extractor\n            if source.lower() == \'twitter\':\n                self.extractor = TwitterExtractor(service, max_posts)\n            elif source.lower() == \'facebook\':\n                self.extractor = FacebookExtractor(service, max_posts)\n            elif source.lower() == \'google_reviews\':\n                self.extractor = GoogleReviewsExtractor(service, max_posts)\n            else:\n                raise ValueError(f"Unsupported source: {source}")\n            \n            # Extract data\n            raw_data = self.extractor.extract_posts(days=days)\n            \n            logger.info(f"Extracted {len(raw_data)} posts from {source}")\n            return raw_data\n            \n        except ExtractionError as e:\n            logger.error(f"Data extraction failed: {e}")\n            raise\n        except Exception as e:\n            logger.error(f"Unexpected error during data extraction: {e}")\n            raise\n    \n    def _preprocess_data(self, raw_data: List[Dict[str, Any]], language: str) -> List[Dict[str, Any]]:\n        """Preprocess extracted data"""\n        processed_data = []\n        \n        for item in raw_data:\n            try:\n                # Extract text content\n                text = item.get(\'text\', item.get(\'message\', item.get(\'content\', \'\')))\n                \n                if not text or not self.validator.validate_text_content(text):\n                    continue\n                \n                # Preprocess text\n                preprocessing_result = self.text_preprocessor.preprocess_text(text, language)\n                \n                # Create processed item\n                processed_item = item.copy()\n                processed_item.update({\n                    \'cleaned_text\': preprocessing_result[\'cleaned\'],\n                    \'tokens\': preprocessing_result[\'tokens\'],\n                    \'language\': preprocessing_result[\'language\'],\n                    \'preprocessing_steps\': preprocessing_result[\'preprocessing_steps\']\n                })\n                \n                processed_data.append(processed_item)\n                \n            except Exception as e:\n                logger.warning(f"Error preprocessing item {item.get(\'id\', \'unknown\')}: {e}")\n                continue\n        \n        logger.info(f"Preprocessed {len(processed_data)} items")\n        return processed_data\n    \n    def _analyze_sentiment(self, processed_data: List[Dict[str, Any]], \n                          sentiment_model: str, language: str) -> List[Dict[str, Any]]:\n        """Analyze sentiment of processed data"""\n        self.sentiment_analyzer = SentimentAnalyzer(sentiment_model, language)\n        \n        sentiment_results = []\n        texts = [item[\'cleaned_text\'] for item in processed_data]\n        \n        # Analyze sentiment for each text\n        for i, (item, text) in enumerate(zip(processed_data, texts)):\n            try:\n                sentiment_result = self.sentiment_analyzer.analyze_sentiment(\n                    text, item.get(\'language\', language)\n                )\n                \n                # Add metadata\n                sentiment_result.update({\n                    \'id\': item.get(\'id\'),\n                    \'original_text\': item.get(\'text\', \'\'),\n                    \'date\': item.get(\'created_at\'),\n                    \'source\': item.get(\'source\'),\n                    \'service\': item.get(\'service\')\n                })\n                \n                sentiment_results.append(sentiment_result)\n                \n            except Exception as e:\n                logger.warning(f"Error analyzing sentiment for item {item.get(\'id\', i)}: {e}")\n                continue\n        \n        logger.info(f"Analyzed sentiment for {len(sentiment_results)} items")\n        return sentiment_results\n    \n    def _extract_keywords(self, processed_data: List[Dict[str, Any]], \n                         keyword_method: str) -> List[Dict[str, Any]]:\n        """Extract keywords from processed data"""\n        self.keyword_extractor = KeywordExtractor(\n            language=\'auto\',  # Use auto-detection\n            max_keywords=50\n        )\n        \n        # Extract texts for keyword analysis\n        texts = [item[\'cleaned_text\'] for item in processed_data if item.get(\'cleaned_text\')]\n        \n        if not texts:\n            logger.warning("No texts available for keyword extraction")\n            return []\n        \n        # Extract keywords\n        keywords = self.keyword_extractor.extract_keywords(texts, keyword_method)\n        \n        # Also extract key phrases\n        key_phrases = self.keyword_extractor.extract_key_phrases(texts)\n        \n        # Combine keywords and phrases\n        all_keywords = keywords + key_phrases\n        \n        # Sort by score and limit\n        all_keywords.sort(key=lambda x: x.get(\'score\', 0), reverse=True)\n        \n        logger.info(f"Extracted {len(all_keywords)} keywords/phrases")\n        return all_keywords[:50]  # Limit to top 50\n    \n    def _analyze_temporal_trends(self, sentiment_results: List[Dict[str, Any]], \n                                processed_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n        """Analyze temporal trends in sentiment"""\n        try:\n            trend_analyzer = SentimentTrendAnalyzer(self.sentiment_analyzer)\n            \n            # Prepare data for temporal analysis\n            texts = []\n            dates = []\n            \n            for result, item in zip(sentiment_results, processed_data):\n                if item.get(\'created_at\'):\n                    texts.append(result.get(\'original_text\', \'\'))\n                    dates.append(item[\'created_at\'])\n            \n            if not texts or not dates:\n                logger.warning("Insufficient temporal data for trend analysis")\n                return {}\n            \n            # Analyze trends\n            trends = trend_analyzer.analyze_temporal_trends(texts, dates)\n            \n            return trends\n            \n        except Exception as e:\n            logger.error(f"Error analyzing temporal trends: {e}")\n            return {}\n    \n    def _generate_summary_statistics(self, sentiment_results: List[Dict[str, Any]],\n                                   keywords: List[Dict[str, Any]], \n                                   raw_data: List[Dict[str, Any]],\n                                   processed_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n        """Generate summary statistics"""\n        try:\n            # Sentiment statistics\n            sentiment_stats = self.sentiment_analyzer.get_sentiment_summary(sentiment_results)\n            \n            # Extraction statistics\n            extraction_stats = self.extractor.get_extraction_stats() if self.extractor else {}\n            \n            # Processing statistics\n            processing_stats = {\n                \'raw_data_count\': len(raw_data),\n                \'processed_data_count\': len(processed_data),\n                \'processing_success_rate\': (len(processed_data) / len(raw_data) * 100) if raw_data else 0\n            }\n            \n            # Keyword statistics\n            keyword_stats = {\n                \'total_keywords\': len(keywords),\n                \'avg_keyword_score\': sum(kw.get(\'score\', 0) for kw in keywords) / len(keywords) if keywords else 0,\n                \'top_keyword\': keywords[0][\'keyword\'] if keywords else None\n            }\n            \n            return {\n                \'sentiment_stats\': sentiment_stats,\n                \'extraction_stats\': extraction_stats,\n                \'processing_stats\': processing_stats,\n                \'keyword_stats\': keyword_stats\n            }\n            \n        except Exception as e:\n            logger.error(f"Error generating summary statistics: {e}")\n            return {}\n    \n    def _compile_results(self, service: str, source: str, days: int, max_posts: int,\n                        raw_data: List[Dict[str, Any]], processed_data: List[Dict[str, Any]],\n                        sentiment_results: List[Dict[str, Any]], keywords: List[Dict[str, Any]],\n                        temporal_analysis: Optional[Dict[str, Any]], \n                        summary_stats: Dict[str, Any]) -> Dict[str, Any]:\n        """Compile all results into final structure"""\n        try:\n            results = {\n                \'metadata\': {\n                    \'service\': service,\n                    \'source\': source,\n                    \'analysis_date\': datetime.now().isoformat(),\n                    \'parameters\': {\n                        \'days\': days,\n                        \'max_posts\': max_posts,\n                        \'language\': \'auto\',\n                        \'sentiment_model\': \'auto\',\n                        \'keyword_method\': \'combined\'\n                    }\n                },\n                \'raw_data\': raw_data,\n                \'processed_data\': processed_data,\n                \'sentiment_results\': sentiment_results,\n                \'sentiment_summary\': summary_stats.get(\'sentiment_stats\', {}),\n                \'keywords\': keywords,\n                \'temporal_data\': temporal_analysis.get(\'detailed_results\', []) if temporal_analysis else [],\n                \'statistics\': summary_stats,\n                \'success\': True\n            }\n            \n            return results\n            \n        except Exception as e:\n            logger.error(f"Error compiling results: {e}")\n            return {\'error\': str(e), \'success\': False}\n    \n    def _generate_visualizations(self, results: Dict[str, Any]):\n        """Generate visualizations for results"""\n        try:\n            # Create output directory for charts\n            output_dir = Path(results[\'metadata\'][\'output_directory\']) / \'charts\'\n            output_dir.mkdir(exist_ok=True)\n            \n            # Generate charts\n            self.charts_generator.save_all_charts(results, str(output_dir))\n            \n            # Generate word clouds\n            wordclouds_dir = Path(results[\'metadata\'][\'output_directory\']) / \'wordclouds\'\n            wordclouds_dir.mkdir(exist_ok=True)\n            self.wordcloud_generator.generate_all_wordclouds(results, str(wordclouds_dir))\n            \n            logger.info("Visualizations generated successfully")\n            \n        except Exception as e:\n            logger.error(f"Error generating visualizations: {e}")\n\n# Convenience function for direct usage\ndef analyze_social_media(service: str, source: str = \'twitter\', days: int = 30, \n                        max_posts: int = 500, **kwargs) -> Dict[str, Any]:\n    """\n    Convenience function to perform social media sentiment analysis\n    \n    Args:\n        service: Service/brand name to analyze\n        source: Social media source (twitter, facebook, google_reviews)\n        days: Number of days to analyze\n        max_posts: Maximum number of posts to extract\n        **kwargs: Additional parameters (language, sentiment_model, etc.)\n    \n    Returns:\n        Analysis results dictionary\n    """\n    analyzer = SocialMediaAnalyzer()\n    \n    return analyzer.analyze(\n        service=service,\n        source=source,\n        days=days,\n        max_posts=max_posts,\n        **kwargs\n    )\n</create-file>\n</function_calls>'}