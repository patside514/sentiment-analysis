{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outil d'Analyse de Sentiment des Médias Sociaux\n",
    "\n",
    "Ce notebook est une adaptation pour Google Colab de l'outil d'analyse de sentiment des médias sociaux. Il permet d'analyser le sentiment des publications sur Twitter, Facebook et Google Reviews.\n",
    "\n",
    "## Fonctionnalités\n",
    "\n",
    "- Extraction de données depuis Twitter (via snscrape)\n",
    "- Analyse de sentiment avec TextBlob et Transformers\n",
    "- Extraction de mots-clés\n",
    "- Visualisation des résultats\n",
    "- Génération de rapports\n",
    "\n",
    "## Comment utiliser ce notebook\n",
    "\n",
    "1. Exécutez les cellules dans l'ordre\n",
    "2. Configurez les paramètres d'analyse dans la section dédiée\n",
    "3. Lancez l'analyse et explorez les résultats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des dépendances\n",
    "\n",
    "Commençons par installer les bibliothèques nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des packages requis\n",
    "!pip install textblob snscrape pandas numpy matplotlib seaborn wordcloud transformers tqdm ipywidgets scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration et importation des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques standard\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any, List, Optional, Tuple, Callable\n",
    "\n",
    "# Bibliothèques de traitement de données\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Bibliothèques NLP\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Bibliothèques de visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Widgets pour l'interface utilisateur\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Configuration de base\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set(style=&quot;whitegrid&quot;)\n",
    "\n",
    "# Téléchargement des ressources NLTK nécessaires\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Définition des classes de base et des utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du logging\n",
    "def setup_logger():\n",
    "    &quot;&quot;&quot;Configure le système de logging&quot;&quot;&quot;\n",
    "    logger = logging.getLogger('SocialMediaAnalyzer')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Handler pour afficher les logs dans le notebook\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "logger = setup_logger()\n",
    "\n",
    "# Classe d'exception personnalisée\n",
    "class ExtractionError(Exception):\n",
    "    &quot;&quot;&quot;Exception levée lors d'erreurs d'extraction de données&quot;&quot;&quot;\n",
    "    pass\n",
    "\n",
    "class RateLimitError(ExtractionError):\n",
    "    &quot;&quot;&quot;Exception levée lors de dépassement de limite de taux&quot;&quot;&quot;\n",
    "    pass\n",
    "\n",
    "class AuthenticationError(ExtractionError):\n",
    "    &quot;&quot;&quot;Exception levée lors d'erreurs d'authentification&quot;&quot;&quot;\n",
    "    pass\n",
    "\n",
    "# Classe de validation des données\n",
    "class DataValidator:\n",
    "    &quot;&quot;&quot;Validateur de données pour l'analyse de sentiment&quot;&quot;&quot;\n",
    "    \n",
    "    def validate_text_content(self, text: str) -> bool:\n",
    "        &quot;&quot;&quot;Valide le contenu textuel&quot;&quot;&quot;\n",
    "        if not text or not isinstance(text, str):\n",
    "            return False\n",
    "        \n",
    "        # Vérifier la longueur minimale (au moins 3 mots)\n",
    "        words = text.split()\n",
    "        if len(words) < 3:\n",
    "            return False\n",
    "        \n",
    "        # Vérifier que le texte n'est pas juste des URLs ou des mentions\n",
    "        cleaned_text = re.sub(r'https?://\\S+|www\\.\\S+|@\\w+', '', text).strip()\n",
    "        if not cleaned_text or len(cleaned_text.split()) < 2:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def validate_date_range(self, start_date: datetime, end_date: datetime) -> bool:\n",
    "        &quot;&quot;&quot;Valide une plage de dates&quot;&quot;&quot;\n",
    "        if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n",
    "            return False\n",
    "        \n",
    "        # La date de début doit être antérieure à la date de fin\n",
    "        if start_date >= end_date:\n",
    "            return False\n",
    "        \n",
    "        # La plage ne doit pas être trop grande (ex: max 1 an)\n",
    "        if (end_date - start_date).days > 365:\n",
    "            return False\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extracteur de données Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe de base pour les extracteurs\n",
    "class BaseExtractor:\n",
    "    &quot;&quot;&quot;Classe de base pour les extracteurs de données&quot;&quot;&quot;\n",
    "    \n",
    "    def __init__(self, service: str, max_posts: int = 500):\n",
    "        self.service = service\n",
    "        self.max_posts = max_posts\n",
    "        self.posts_extracted = 0\n",
    "        self.errors_count = 0\n",
    "        self.validator = DataValidator()\n",
    "    \n",
    "    def extract_posts(self, days: int = 30, **kwargs) -> List[Dict[str, Any]]:\n",
    "        &quot;&quot;&quot;Méthode à implémenter dans les sous-classes&quot;&quot;&quot;\n",
    "        raise NotImplementedError(&quot;Cette méthode doit être implémentée dans les sous-classes&quot;)\n",
    "    \n",
    "    def _calculate_date_range(self, days: int) -> Tuple[datetime, datetime]:\n",
    "        &quot;&quot;&quot;Calcule la plage de dates pour l'extraction&quot;&quot;&quot;\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "        return start_date, end_date\n",
    "    \n",
    "    def _validate_and_clean_post(self, post: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "        &quot;&quot;&quot;Valide et nettoie un post&quot;&quot;&quot;\n",
    "        # Vérifier que le post contient du texte\n",
    "        text = post.get('text', '')\n",
    "        if not self.validator.validate_text_content(text):\n",
    "            return None\n",
    "        \n",
    "        # Nettoyer le texte des URLs et autres éléments non pertinents\n",
    "        post['text'] = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        \n",
    "        return post\n",
    "    \n",
    "    def _normalize_post_structure(self, post: Dict[str, Any], source: str) -> Dict[str, Any]:\n",
    "        &quot;&quot;&quot;Normalise la structure d'un post pour un traitement uniforme&quot;&quot;&quot;\n",
    "        normalized = {\n",
    "            'id': post.get('id', str(hash(str(post)))),\n",
    "            'text': post.get('text', ''),\n",
    "            'created_at': post.get('created_at', datetime.now().isoformat()),\n",
    "            'source': source,\n",
    "            'service': self.service\n",
    "        }\n",
    "        \n",
    "        # Ajouter des métriques si disponibles\n",
    "        if 'likes' in post:\n",
    "            normalized['likes'] = post['likes']\n",
    "        if 'retweets' in post:\n",
    "            normalized['shares'] = post['retweets']\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def _handle_extraction_error(self, error: Exception, context: str):\n",
    "        &quot;&quot;&quot;Gère les erreurs d'extraction&quot;&quot;&quot;\n",
    "        self.errors_count += 1\n",
    "        \n",
    "        if isinstance(error, RateLimitError):\n",
    "            logger.warning(f&quot;Limite de taux atteinte pendant {context}: {error}&quot;)\n",
    "            raise\n",
    "        elif isinstance(error, AuthenticationError):\n",
    "            logger.error(f&quot;Erreur d'authentification pendant {context}: {error}&quot;)\n",
    "            raise\n",
    "        else:\n",
    "            logger.error(f&quot;Erreur pendant {context}: {error}&quot;)\n",
    "            raise ExtractionError(f&quot;Erreur d'extraction: {error}&quot;)\n",
    "    \n",
    "    def _rate_limit_delay(self, seconds: float = 1.0):\n",
    "        &quot;&quot;&quot;Ajoute un délai pour respecter les limites de taux&quot;&quot;&quot;\n",
    "        time.sleep(seconds)\n",
    "    \n",
    "    def get_extraction_stats(self) -> Dict[str, Any]:\n",
    "        &quot;&quot;&quot;Retourne les statistiques d'extraction&quot;&quot;&quot;\n",
    "        return {\n",
    "            'service': self.service,\n",
    "            'posts_extracted': self.posts_extracted,\n",
    "            'errors_count': self.errors_count,\n",
    "            'success_rate': (self.posts_extracted / (self.posts_extracted + self.errors_count) * 100) \n",
    "                if (self.posts_extracted + self.errors_count) > 0 else 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracteur Twitter\n",
    "class TwitterExtractor(BaseExtractor):\n",
    "    &quot;&quot;&quot;Extracteur de données Twitter utilisant snscrape&quot;&quot;&quot;\n",
    "    \n",
    "    def __init__(self, service: str, max_posts: int = 500):\n",
    "        super().__init__(service, max_posts)\n",
    "        \n",
    "        # Vérifier si snscrape est disponible\n",
    "        try:\n",
    "            import snscrape.modules.twitter as sntwitter\n",
    "            self.snscrape_available = True\n",
    "        except ImportError:\n",
    "            self.snscrape_available = False\n",
    "            logger.warning(&quot;snscrape n'est pas disponible. L'extraction Twitter sera limitée.&quot;)\n",
    "    \n",
    "    def extract_posts(self, days: int = 30, **kwargs) -> List[Dict[str, Any]]:\n",
    "        &quot;&quot;&quot;Extrait les tweets avec snscrape&quot;&quot;&quot;\n",
    "        logger.info(f&quot;Extraction des tweets pour '{self.service}' des {days} derniers jours&quot;)\n",
    "        \n",
    "        try:\n",
    "            if self.snscrape_available:\n",
    "                return self._extract_with_snscrape(days, **kwargs)\n",
    "            else:\n",
    "                raise ExtractionError(&quot;Aucune méthode d'extraction Twitter disponible&quot;)\n",
    "                \n",
    "        except Exception as e:\n",
    "            self._handle_extraction_error(e, &quot;Extraction Twitter&quot;)\n",
    "            return []\n",
    "    \n",
    "    def _extract_with_snscrape(self, days: int, **kwargs) -> List[Dict[str, Any]]:\n",
    "        &quot;&quot;&quot;Extrait les tweets en utilisant snscrape&quot;&quot;&quot;\n",
    "        import snscrape.modules.twitter as sntwitter\n",
    "        \n",
    "        posts = []\n",
    "        start_date, end_date = self._calculate_date_range(days)\n",
    "        \n",
    "        try:\n",
    "            query = self._build_search_query()\n",
    "            \n",
    "            # Construire la requête snscrape avec la plage de dates\n",
    "            since_date = start_date.strftime(&quot;%Y-%m-%d&quot;)\n",
    "            until_date = end_date.strftime(&quot;%Y-%m-%d&quot;)\n",
    "            snscrape_query = f&quot;{query} since:{since_date} until:{until_date}&quot;\n",
    "            \n",
    "            logger.info(f&quot;Utilisation de snscrape avec la requête: {snscrape_query}&quot;)\n",
    "            \n",
    "            # Recherche de tweets avec barre de progression\n",
    "            for i, tweet in enumerate(tqdm(\n",
    "                sntwitter.TwitterSearchScraper(snscrape_query).get_items(),\n",
    "                desc=&quot;Extraction des tweets&quot;,\n",
    "                total=self.max_posts\n",
    "            )):\n",
    "                if len(posts) >= self.max_posts:\n",
    "                    break\n",
    "                \n",
    "                processed_tweet = self._process_snscrape_tweet(tweet)\n",
    "                if processed_tweet:\n",
    "                    posts.append(processed_tweet)\n",
    "                    self.posts_extracted += 1\n",
    "                \n",
    "                # Suivi de la progression\n",
    "                if i % 50 == 0 and i > 0:\n",
    "                    logger.info(f&quot;Traité {i} tweets, extrait {len(posts)}&quot;)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur snscrape: {e}&quot;)\n",
    "            raise ExtractionError(f&quot;L'extraction snscrape a échoué: {e}&quot;)\n",
    "        \n",
    "        logger.info(f&quot;Extrait {len(posts)} tweets via snscrape&quot;)\n",
    "        return posts\n",
    "    \n",
    "    def _build_search_query(self) -> str:\n",
    "        &quot;&quot;&quot;Construit la requête de recherche Twitter&quot;&quot;&quot;\n",
    "        service_terms = self.service.lower().split()\n",
    "        \n",
    "        # Créer une requête avec le nom du service et ses variations\n",
    "        query_parts = []\n",
    "        for term in service_terms:\n",
    "            query_parts.append(term)\n",
    "            query_parts.append(f&quot;#{term}&quot;)\n",
    "            query_parts.append(f&quot;@{term}&quot;)\n",
    "        \n",
    "        # Joindre les parties avec OR\n",
    "        query = &quot; OR &quot;.join(query_parts)\n",
    "        \n",
    "        # Exclure les retweets pour du contenu original\n",
    "        query += &quot; -is:retweet&quot;\n",
    "        \n",
    "        return query\n",
    "    \n",
    "    def _process_snscrape_tweet(self, tweet) -> Optional[Dict[str, Any]]:\n",
    "        &quot;&quot;&quot;Traite un objet tweet snscrape&quot;&quot;&quot;\n",
    "        try:\n",
    "            tweet_data = {\n",
    "                'id': str(tweet.id),\n",
    "                'text': tweet.content,\n",
    "                'created_at': tweet.date.isoformat(),\n",
    "                'username': tweet.user.username,\n",
    "                'display_name': tweet.user.displayname,\n",
    "                'likes': tweet.likeCount,\n",
    "                'retweets': tweet.retweetCount,\n",
    "                'replies': tweet.replyCount,\n",
    "                'quotes': tweet.quoteCount,\n",
    "                'lang': getattr(tweet, 'lang', 'unknown')\n",
    "            }\n",
    "            \n",
    "            # Valider et nettoyer\n",
    "            validated_tweet = self._validate_and_clean_post(tweet_data)\n",
    "            if validated_tweet:\n",
    "                return self._normalize_post_structure(validated_tweet, 'twitter')\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur de traitement du tweet snscrape: {e}&quot;)\n",
    "            self.errors_count += 1\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prétraitement de texte et analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préprocesseur de texte\n",
    "class TextPreprocessor:\n",
    "    &quot;&quot;&quot;Prétraitement de texte pour l'analyse de sentiment&quot;&quot;&quot;\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stopwords_en = set(stopwords.words('english'))\n",
    "        self.stopwords_fr = set(stopwords.words('french'))\n",
    "    \n",
    "    def preprocess_text(self, text: str, language: str = 'auto') -> Dict[str, Any]:\n",
    "        &quot;&quot;&quot;Prétraite un texte pour l'analyse&quot;&quot;&quot;\n",
    "        if not text:\n",
    "            return {'cleaned': '', 'tokens': [], 'language': 'unknown', 'preprocessing_steps': []}\n",
    "        \n",
    "        preprocessing_steps = []\n",
    "        \n",
    "        # Détection de la langue si nécessaire\n",
    "        if language == 'auto':\n",
    "            language = self._detect_language(text)\n",
    "        \n",
    "        # Conversion en minuscules\n",
    "        text = text.lower()\n",
    "        preprocessing_steps.append('lowercase')\n",
    "        \n",
    "        # Suppression des URLs\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        preprocessing_steps.append('remove_urls')\n",
    "        \n",
    "        # Suppression des mentions (@user)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        preprocessing_steps.append('remove_mentions')\n",
    "        \n",
    "        # Suppression des hashtags (#topic)\n",
    "        text = re.sub(r'#\\w+', '', text)\n",
    "        preprocessing_steps.append('remove_hashtags')\n",
    "        \n",
    "        # Suppression des caractères spéciaux et chiffres\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        preprocessing_steps.append('remove_special_chars')\n",
    "        \n",
    "        # Tokenisation\n",
    "        tokens = word_tokenize(text)\n",
    "        preprocessing_steps.append('tokenize')\n",
    "        \n",
    "        # Suppression des stop words\n",
    "        if language == 'french':\n",
    "            tokens = [word for word in tokens if word not in self.stopwords_fr]\n",
    "        else:  # default to English\n",
    "            tokens = [word for word in tokens if word not in self.stopwords_en]\n",
    "        preprocessing_steps.append('remove_stopwords')\n",
    "        \n",
    "        # Reconstruction du texte nettoyé\n",
    "        cleaned_text = ' '.join(tokens)\n",
    "        \n",
    "        return {\n",
    "            'cleaned': cleaned_text,\n",
    "            'tokens': tokens,\n",
    "            'language': language,\n",
    "            'preprocessing_steps': preprocessing_steps\n",
    "        }\n",
    "    \n",
    "    def _detect_language(self, text: str) -> str:\n",
    "        &quot;&quot;&quot;Détection simple de la langue pour l'analyse de sentiment&quot;&quot;&quot;\n",
    "        # Heuristique simple basée sur les mots courants\n",
    "        french_words = ['le', 'la', 'les', 'un', 'une', 'de', 'du', 'des', 'et', 'est', 'sont']\n",
    "        english_words = ['the', 'and', 'is', 'are', 'in', 'on', 'at', 'to', 'for', 'of']\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        french_score = sum(1 for word in french_words if f&quot; {word} &quot; in f&quot; {text_lower} &quot;)\n",
    "        english_score = sum(1 for word in english_words if f&quot; {word} &quot; in f&quot; {text_lower} &quot;)\n",
    "        \n",
    "        if french_score > english_score:\n",
    "            return 'french'\n",
    "        else:\n",
    "            return 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyseur de sentiment\n",
    "class SentimentAnalyzer:\n",
    "    &quot;&quot;&quot;Analyseur de sentiment multi-modèles&quot;&quot;&quot;\n",
    "    \n",
    "    def __init__(self, model_type: str = 'auto', language: str = 'auto'):\n",
    "        self.model_type = model_type\n",
    "        self.language = language\n",
    "        self.models = {}\n",
    "        self._setup_models()\n",
    "    \n",
    "    def _setup_models(self):\n",
    "        &quot;&quot;&quot;Configure les modèles d'analyse de sentiment&quot;&quot;&quot;\n",
    "        try:\n",
    "            # Vérifier si transformers est disponible\n",
    "            try:\n",
    "                from transformers import pipeline\n",
    "                # Charger le modèle transformers pour l'analyse de sentiment\n",
    "                self.models['transformers'] = pipeline(\n",
    "                    &quot;sentiment-analysis&quot;,\n",
    "                    model=&quot;cardiffnlp/twitter-roberta-base-sentiment-latest&quot;,\n",
    "                    tokenizer=&quot;cardiffnlp/twitter-roberta-base-sentiment-latest&quot;\n",
    "                )\n",
    "                logger.info(&quot;Modèle Transformers chargé pour l'analyse de sentiment&quot;)\n",
    "            except ImportError:\n",
    "                logger.warning(&quot;La bibliothèque transformers n'est pas disponible&quot;)\n",
    "            \n",
    "            logger.info(&quot;Analyse de sentiment TextBlob disponible&quot;)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur lors de la configuration des modèles de sentiment: {e}&quot;)\n",
    "    \n",
    "    def analyze_sentiment(self, text: str, language: Optional[str] = None) -> Dict[str, Any]:\n",
    "        &quot;&quot;&quot;Analyse le sentiment d'un texte&quot;&quot;&quot;\n",
    "        if not text or not isinstance(text, str):\n",
    "            return self._get_neutral_result()\n",
    "        \n",
    "        try:\n",
    "            lang = language or self.language or self._detect_language(text)\n",
    "            \n",
    "            # Choisir la méthode d'analyse\n",
    "            if self.model_type == 'transformers' and 'transformers' in self.models:\n",
    "                result = self._analyze_with_transformers(text)\n",
    "            elif lang == 'french':\n",
    "                result = self._analyze_with_textblob_fr(text)\n",
    "            else:\n",
    "                result = self._analyze_with_textblob_en(text)\n",
    "            \n",
    "            # Ajouter des métadonnées\n",
    "            result.update({\n",
    "                'text': text[:100] + '..' if len(text) > 100 else text,\n",
    "                'language': lang,\n",
    "                'model_used': self.model_type\n",
    "            })\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur d'analyse de sentiment: {e}&quot;)\n",
    "            return self._get_neutral_result()\n",
    "    \n",
    "    def analyze_batch(self, texts: List[str], language: Optional[str] = None) -> List[Dict[str, Any]]:\n",
    "        &quot;&quot;&quot;Analyse le sentiment pour plusieurs textes&quot;&quot;&quot;\n",
    "        results = []\n",
    "        \n",
    "        for text in tqdm(texts, desc=&quot;Analyse de sentiment&quot;):\n",
    "            result = self.analyze_sentiment(text, language)\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _analyze_with_transformers(self, text: str) -> Dict[str, Any]:\n",
    "        &quot;&quot;&quot;Analyse le sentiment avec Transformers&quot;&quot;&quot;\n",
    "        try:\n",
    "            # Tronquer le texte si trop long\n",
    "            max_length = 512\n",
    "            if len(text) > max_length:\n",
    "                text = text[:max_length]\n",
    "            \n",
    "            result = self.models['transformers'](text)[0]\n",
    "            \n",
    "            label = result['label'].lower()\n",
    "            score = result['score']\n",
    "            \n",
    "            # Mapper au sentiment standard\n",
    "            if 'positive' in label:\n",
    "                sentiment = 'positive'\n",
    "                polarity = score\n",
    "            elif 'negative' in label:\n",
    "                sentiment = 'negative'\n",
    "                polarity = -score\n",
    "            else:\n",
    "                sentiment = 'neutral'\n",
    "                polarity = 0\n",
    "            \n",
    "            return {\n",
    "                'sentiment': sentiment,\n",
    "                'polarity': polarity,\n",
    "                'confidence': score,\n",
    "                'raw_label': result['label'],\n",
    "                'method': 'transformers'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur d'analyse Transformers: {e}&quot;)\n",
    "            return self._analyze_with_textblob_en(text)  # Fallback\n",
    "    \n",
    "    def _analyze_with_textblob_en(self, text: str) -> Dict[str, Any]:\n",
    "        &quot;&quot;&quot;Analyse le sentiment avec TextBlob (Anglais)&quot;&quot;&quot;\n",
    "        try:\n",
    "            blob = TextBlob(text)\n",
    "            polarity = blob.sentiment.polarity\n",
    "            subjectivity = blob.sentiment.subjectivity\n",
    "            \n",
    "            # Classifier le sentiment\n",
    "            if polarity > 0.1:\n",
    "                sentiment = 'positive'\n",
    "            elif polarity < -0.1:\n",
    "                sentiment = 'negative'\n",
    "            else:\n",
    "                sentiment = 'neutral'\n",
    "            \n",
    "            return {\n",
    "                'sentiment': sentiment,\n",
    "                'polarity': polarity,\n",
    "                'subjectivity': subjectivity,\n",
    "                'confidence': abs(polarity),\n",
    "                'method': 'textblob_en'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur d'analyse TextBlob Anglais: {e}&quot;)\n",
    "            return self._get_neutral_result()\n",
    "    \n",
    "    def _analyze_with_textblob_fr(self, text: str) -> Dict[str, Any]:\n",
    "        &quot;&quot;&quot;Analyse le sentiment avec TextBlob (Français)&quot;&quot;&quot;\n",
    "        try:\n",
    "            # TextBlob fonctionne avec le texte français, bien que la précision puisse varier\n",
    "            blob = TextBlob(text)\n",
    "            polarity = blob.sentiment.polarity\n",
    "            subjectivity = blob.sentiment.subjectivity\n",
    "            \n",
    "            # Ajuster les seuils pour le français (plus conservateur)\n",
    "            if polarity > 0.2:\n",
    "                sentiment = 'positive'\n",
    "                confidence = polarity\n",
    "            elif polarity < -0.2:\n",
    "                sentiment = 'negative'\n",
    "                confidence = abs(polarity)\n",
    "            else:\n",
    "                sentiment = 'neutral'\n",
    "                confidence = 1.0 - abs(polarity)  # Confiance plus élevée pour neutre\n",
    "            \n",
    "            return {\n",
    "                'sentiment': sentiment,\n",
    "                'polarity': polarity,\n",
    "                'subjectivity': subjectivity,\n",
    "                'confidence': confidence,\n",
    "                'method': 'textblob_fr'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur d'analyse TextBlob Français: {e}&quot;)\n",
    "            return self._analyze_with_textblob_en(text)  # Fallback vers l'anglais\n",
    "    \n",
    "    def _detect_language(self, text: str) -> str:\n",
    "        &quot;&quot;&quot;Détection simple de la langue pour l'analyse de sentiment&quot;&quot;&quot;\n",
    "        # Heuristique simple basée sur les mots courants\n",
    "        french_words = ['le', 'la', 'les', 'un', 'une', 'de', 'du', 'des', 'et', 'est', 'sont']\n",
    "        english_words = ['the', 'and', 'is', 'are', 'in', 'on', 'at', 'to', 'for', 'of']\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        french_score = sum(1 for word in french_words if f&quot; {word} &quot; in f&quot; {text_lower} &quot;)\n",
    "        english_score = sum(1 for word in english_words if f&quot; {word} &quot; in f&quot; {text_lower} &quot;)\n",
    "        \n",
    "        if french_score > english_score:\n",
    "            return 'french'\n",
    "        else:\n",
    "            return 'english'\n",
    "    \n",
    "    def _get_neutral_result(self) -> Dict[str, Any]:\n",
    "        &quot;&quot;&quot;Retourne un résultat de sentiment neutre&quot;&quot;&quot;\n",
    "        return {\n",
    "            'sentiment': 'neutral',\n",
    "            'polarity': 0.0,\n",
    "            'subjectivity': 0.0,\n",
    "            'confidence': 0.0,\n",
    "            'method': 'fallback',\n",
    "            'error': 'Analyse échouée'\n",
    "        }\n",
    "    \n",
    "    def get_sentiment_summary(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        &quot;&quot;&quot;Obtient des statistiques de résumé pour les résultats d'analyse de sentiment&quot;&quot;&quot;\n",
    "        if not results:\n",
    "            return {\n",
    "                'total': 0,\n",
    "                'positive': 0,\n",
    "                'negative': 0,\n",
    "                'neutral': 0,\n",
    "                'percentages': {\n",
    "                    'positive': 0,\n",
    "                    'negative': 0,\n",
    "                    'neutral': 0\n",
    "                },\n",
    "                'average_polarity': 0.0,\n",
    "                'average_confidence': 0.0\n",
    "            }\n",
    "        \n",
    "        total = len(results)\n",
    "        positive = sum(1 for r in results if r['sentiment'] == 'positive')\n",
    "        negative = sum(1 for r in results if r['sentiment'] == 'negative')\n",
    "        neutral = sum(1 for r in results if r['sentiment'] == 'neutral')\n",
    "        \n",
    "        avg_polarity = np.mean([r['polarity'] for r in results])\n",
    "        avg_confidence = np.mean([r.get('confidence', 0) for r in results])\n",
    "        \n",
    "        return {\n",
    "            'total': total,\n",
    "            'positive': positive,\n",
    "            'negative': negative,\n",
    "            'neutral': neutral,\n",
    "            'percentages': {\n",
    "                'positive': round(positive / total * 100, 2),\n",
    "                'negative': round(negative / total * 100, 2),\n",
    "                'neutral': round(neutral / total * 100, 2)\n",
    "            },\n",
    "            'average_polarity': round(avg_polarity, 3),\n",
    "            'average_confidence': round(avg_confidence, 3)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extraction de mots-clés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracteur de mots-clés\n",
    "class KeywordExtractor:\n",
    "    &quot;&quot;&quot;Extracteur de mots-clés utilisant plusieurs méthodes&quot;&quot;&quot;\n",
    "    \n",
    "    def __init__(self, language: str = 'auto', max_keywords: int = 50):\n",
    "        self.language = language\n",
    "        self.max_keywords = max_keywords\n",
    "        self.stopwords_en = set(stopwords.words('english'))\n",
    "        self.stopwords_fr = set(stopwords.words('french'))\n",
    "    \n",
    "    def extract_keywords(self, texts: List[str], method: str = 'combined') -> List[Dict[str, Any]]:\n",
    "        &quot;&quot;&quot;Extrait les mots-clés des textes&quot;&quot;&quot;\n",
    "        if not texts:\n",
    "            return []\n",
    "        \n",
    "        # Déterminer la méthode d'extraction\n",
    "        if method == 'tfidf':\n",
    "            return self._extract_with_tfidf(texts)\n",
    "        elif method == 'frequency':\n",
    "            return self._extract_with_frequency(texts)\n",
    "        else:  # combined\n",
    "            tfidf_keywords = self._extract_with_tfidf(texts)\n",
    "            freq_keywords = self._extract_with_frequency(texts)\n",
    "            return self._combine_keyword_results(tfidf_keywords, freq_keywords)\n",
    "    \n",
    "    def _extract_with_tfidf(self, texts: List[str]) -> List[Dict[str, Any]]:\n",
    "        &quot;&quot;&quot;Extrait les mots-clés en utilisant TF-IDF&quot;&quot;&quot;\n",
    "        try:\n",
    "            # Déterminer la langue pour les stop words\n",
    "            if self.language == 'auto':\n",
    "                # Utiliser l'anglais par défaut pour TF-IDF\n",
    "                stop_words = self.stopwords_en\n",
    "            elif self.language == 'french':\n",
    "                stop_words = self.stopwords_fr\n",
    "            else:\n",
    "                stop_words = self.stopwords_en\n",
    "            \n",
    "            # Configurer le vectoriseur TF-IDF\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                max_features=100,\n",
    "                stop_words=list(stop_words),\n",
    "                ngram_range=(1, 2),  # Unigrammes et bigrammes\n",
    "                min_df=2  # Ignorer les termes qui apparaissent dans moins de 2 documents\n",
    "            )\n",
    "            \n",
    "            # Calculer les scores TF-IDF\n",
    "            tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # Calculer les scores moyens pour chaque terme\n",
    "            tfidf_scores = np.array(tfidf_matrix.mean(axis=0)).flatten()\n",
    "            \n",
    "            # Créer un dictionnaire de scores\n",
    "            keyword_scores = {feature_names[i]: tfidf_scores[i] for i in range(len(feature_names))}\n",
    "            \n",
    "            # Trier par score et convertir en liste de dictionnaires\n",
    "            sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Limiter au nombre maximum de mots-clés\n",
    "            top_keywords = sorted_keywords[:self.max_keywords]\n",
    "            \n",
    "            # Convertir en format standard\n",
    "            result = [\n",
    "                {\n",
    "                    'keyword': kw,\n",
    "                    'score': float(score),\n",
    "                    'method': 'tfidf',\n",
    "                    'frequency': self._count_term_frequency(kw, texts)\n",
    "                }\n",
    "                for kw, score in top_keywords\n",
    "            ]\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur d'extraction TF-IDF: {e}&quot;)\n",
    "            return []\n",
    "    \n",
    "    def _extract_with_frequency(self, texts: List[str]) -> List[Dict[str, Any]]:\n",
    "        &quot;&quot;&quot;Extrait les mots-clés en utilisant la fréquence des termes&quot;&quot;&quot;\n",
    "        try:\n",
    "            # Déterminer la langue pour les stop words\n",
    "            if self.language == 'auto':\n",
    "                # Utiliser l'anglais par défaut\n",
    "                stop_words = self.stopwords_en\n",
    "            elif self.language == 'french':\n",
    "                stop_words = self.stopwords_fr\n",
    "            else:\n",
    "                stop_words = self.stopwords_en\n",
    "            \n",
    "            # Configurer le vectoriseur de comptage\n",
    "            vectorizer = CountVectorizer(\n",
    "                max_features=100,\n",
    "                stop_words=list(stop_words),\n",
    "                ngram_range=(1, 2)  # Unigrammes et bigrammes\n",
    "            )\n",
    "            \n",
    "            # Calculer les fréquences\n",
    "            count_matrix = vectorizer.fit_transform(texts)\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # Calculer les fréquences totales\n",
    "            count_scores = np.array(count_matrix.sum(axis=0)).flatten()\n",
    "            \n",
    "            # Créer un dictionnaire de fréquences\n",
    "            keyword_counts = {feature_names[i]: int(count_scores[i]) for i in range(len(feature_names))}\n",
    "            \n",
    "            # Trier par fréquence et convertir en liste de dictionnaires\n",
    "            sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Limiter au nombre maximum de mots-clés\n",
    "            top_keywords = sorted_keywords[:self.max_keywords]\n",
    "            \n",
    "            # Normaliser les scores\n",
    "            max_count = max([count for _, count in top_keywords]) if top_keywords else 1\n",
    "            \n",
    "            # Convertir en format standard\n",
    "            result = [\n",
    "                {\n",
    "                    'keyword': kw,\n",
    "                    'score': count / max_count,  # Score normalisé\n",
    "                    'method': 'frequency',\n",
    "                    'frequency': count\n",
    "                }\n",
    "                for kw, count in top_keywords\n",
    "            ]\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur d'extraction par fréquence: {e}&quot;)\n",
    "            return []\n",
    "    \n",
    "    def extract_key_phrases(self, texts: List[str]) -> List[Dict[str, Any]]:\n",
    "        &quot;&quot;&quot;Extrait les phrases clés des textes&quot;&quot;&quot;\n",
    "        # Méthode simplifiée pour extraire des phrases clés\n",
    "        # Dans une implémentation complète, on pourrait utiliser des méthodes plus avancées\n",
    "        try:\n",
    "            # Combiner tous les textes\n",
    "            combined_text = ' '.join(texts)\n",
    "            \n",
    "            # Extraire des phrases (séquences de 3-5 mots)\n",
    "            words = combined_text.split()\n",
    "            phrases = []\n",
    "            \n",
    "            for i in range(len(words) - 3):\n",
    "                phrase = ' '.join(words[i:i+3])  # Phrases de 3 mots\n",
    "                if len(phrase) > 10:  # Ignorer les phrases trop courtes\n",
    "                    phrases.append(phrase)\n",
    "            \n",
    "            # Compter les occurrences\n",
    "            phrase_counts = {}\n",
    "            for phrase in phrases:\n",
    "                phrase_counts[phrase] = phrase_counts.get(phrase, 0) + 1\n",
    "            \n",
    "            # Trier par fréquence\n",
    "            sorted_phrases = sorted(phrase_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Limiter et normaliser\n",
    "            top_phrases = sorted_phrases[:20]  # Limiter à 20 phrases\n",
    "            max_count = max([count for _, count in top_phrases]) if top_phrases else 1\n",
    "            \n",
    "            # Convertir en format standard\n",
    "            result = [\n",
    "                {\n",
    "                    'keyword': phrase,\n",
    "                    'score': count / max_count,  # Score normalisé\n",
    "                    'method': 'phrase',\n",
    "                    'frequency': count\n",
    "                }\n",
    "                for phrase, count in top_phrases\n",
    "            ]\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur d'extraction de phrases clés: {e}&quot;)\n",
    "            return []\n",
    "    \n",
    "    def _combine_keyword_results(self, tfidf_keywords: List[Dict[str, Any]], \n",
    "                               freq_keywords: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        &quot;&quot;&quot;Combine les résultats de différentes méthodes d'extraction&quot;&quot;&quot;\n",
    "        # Créer un dictionnaire pour fusionner les résultats\n",
    "        combined = {}\n",
    "        \n",
    "        # Ajouter les mots-clés TF-IDF\n",
    "        for kw in tfidf_keywords:\n",
    "            combined[kw['keyword']] = {\n",
    "                'keyword': kw['keyword'],\n",
    "                'tfidf_score': kw['score'],\n",
    "                'frequency': kw['frequency'],\n",
    "                'methods': ['tfidf']\n",
    "            }\n",
    "        \n",
    "        # Ajouter ou mettre à jour avec les mots-clés de fréquence\n",
    "        for kw in freq_keywords:\n",
    "            if kw['keyword'] in combined:\n",
    "                combined[kw['keyword']]['freq_score'] = kw['score']\n",
    "                combined[kw['keyword']]['methods'].append('frequency')\n",
    "            else:\n",
    "                combined[kw['keyword']] = {\n",
    "                    'keyword': kw['keyword'],\n",
    "                    'freq_score': kw['score'],\n",
    "                    'frequency': kw['frequency'],\n",
    "                    'methods': ['frequency']\n",
    "                }\n",
    "        \n",
    "        # Calculer un score combiné\n",
    "        for keyword, data in combined.items():\n",
    "            tfidf_score = data.get('tfidf_score', 0)\n",
    "            freq_score = data.get('freq_score', 0)\n",
    "            \n",
    "            # Score combiné: moyenne pondérée (TF-IDF a plus de poids)\n",
    "            if 'tfidf_score' in data and 'freq_score' in data:\n",
    "                combined_score = (tfidf_score * 0.7) + (freq_score * 0.3)\n",
    "            elif 'tfidf_score' in data:\n",
    "                combined_score = tfidf_score\n",
    "            else:\n",
    "                combined_score = freq_score\n",
    "            \n",
    "            data['score'] = combined_score\n",
    "        \n",
    "        # Convertir en liste et trier par score combiné\n",
    "        result = list(combined.values())\n",
    "        result.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # Limiter au nombre maximum de mots-clés\n",
    "        return result[:self.max_keywords]\n",
    "    \n",
    "    def _count_term_frequency(self, term: str, texts: List[str]) -> int:\n",
    "        &quot;&quot;&quot;Compte la fréquence d'un terme dans les textes&quot;&quot;&quot;\n",
    "        count = 0\n",
    "        for text in texts:\n",
    "            count += text.lower().count(term.lower())\n",
    "        return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisation des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générateur de graphiques\n",
    "class ChartsGenerator:\n",
    "    &quot;&quot;&quot;Génère divers graphiques pour l'analyse de sentiment&quot;&quot;&quot;\n",
    "    \n",
    "    def __init__(self, style: str = None):\n",
    "        self.style = style or 'seaborn-v0_8-whitegrid'\n",
    "        plt.style.use(self.style)\n",
    "        self.colors = sns.color_palette(&quot;Set2&quot;)\n",
    "    \n",
    "    def create_sentiment_pie_chart(self, sentiment_summary: Dict[str, Any], \n",
    "                                title: str = &quot;Distribution des sentiments&quot;) -> plt.Figure:\n",
    "        &quot;&quot;&quot;Crée un graphique circulaire pour la distribution des sentiments&quot;&quot;&quot;\n",
    "        try:\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            \n",
    "            # Préparer les données\n",
    "            labels = ['Positif', 'Négatif', 'Neutre']\n",
    "            sizes = [\n",
    "                sentiment_summary['percentages']['positive'],\n",
    "                sentiment_summary['percentages']['negative'],\n",
    "                sentiment_summary['percentages']['neutral']\n",
    "            ]\n",
    "            colors = ['#2E8B57', '#DC143C', '#808080']  # Vert, Rouge, Gris\n",
    "            \n",
    "            # Créer le graphique circulaire\n",
    "            wedges, texts, autotexts = ax.pie(\n",
    "                sizes,\n",
    "                labels=labels,\n",
    "                colors=colors,\n",
    "                autopct='%1.1f%%',\n",
    "                startangle=90,\n",
    "                explode=(0.05, 0.05, 0.05)\n",
    "            )\n",
    "            \n",
    "            # Personnaliser\n",
    "            ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "            \n",
    "            # Style du texte\n",
    "            for autotext in autotexts:\n",
    "                autotext.set_color('white')\n",
    "                autotext.set_fontweight('bold')\n",
    "                autotext.set_fontsize(12)\n",
    "            \n",
    "            # Ajouter des informations de comptage\n",
    "            total = sentiment_summary['total']\n",
    "            counts = [\n",
    "                sentiment_summary['positive'],\n",
    "                sentiment_summary['negative'],\n",
    "                sentiment_summary['neutral']\n",
    "            ]\n",
    "            \n",
    "            legend_labels = [\n",
    "                f&quot;{label}: {count} ({size:.1f}%)&quot; \n",
    "                for label, count, size in zip(labels, counts, sizes)\n",
    "            ]\n",
    "            \n",
    "            ax.legend(\n",
    "                wedges, legend_labels,\n",
    "                title=&quot;Comptage des sentiments&quot;,\n",
    "                loc=&quot;center left&quot;,\n",
    "                bbox_to_anchor=(1, 0, 0.5, 1)\n",
    "            )\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            return fig\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur lors de la création du graphique circulaire des sentiments: {e}&quot;)\n",
    "            return self._create_error_chart(&quot;Graphique circulaire des sentiments&quot;)\n",
    "    \n",
    "    def create_sentiment_bar_chart(self, sentiment_summary: Dict[str, Any],\n",
    "                                title: str = &quot;Résultats de l'analyse de sentiment&quot;) -> plt.Figure:\n",
    "        &quot;&quot;&quot;Crée un graphique à barres pour l'analyse de sentiment&quot;&quot;&quot;\n",
    "        try:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # Graphique de gauche: Comptages\n",
    "            sentiments = ['Positif', 'Négatif', 'Neutre']\n",
    "            counts = [\n",
    "                sentiment_summary['positive'],\n",
    "                sentiment_summary['negative'],\n",
    "                sentiment_summary['neutral']\n",
    "            ]\n",
    "            colors = ['#2E8B57', '#DC143C', '#808080']\n",
    "            \n",
    "            bars1 = ax1.bar(sentiments, counts, color=colors, alpha=0.8)\n",
    "            ax1.set_title('Comptage des sentiments', fontsize=14, fontweight='bold')\n",
    "            ax1.set_ylabel('Nombre de publications')\n",
    "            \n",
    "            # Ajouter des étiquettes de valeur sur les barres\n",
    "            for bar, count in zip(bars1, counts):\n",
    "                height = bar.get_height()\n",
    "                ax1.text(\n",
    "                    bar.get_x() + bar.get_width()/2., height + max(counts)*0.01,\n",
    "                    f'{count}', ha='center', va='bottom', fontweight='bold'\n",
    "                )\n",
    "            \n",
    "            # Graphique de droite: Pourcentages\n",
    "            percentages = [\n",
    "                sentiment_summary['percentages']['positive'],\n",
    "                sentiment_summary['percentages']['negative'],\n",
    "                sentiment_summary['percentages']['neutral']\n",
    "            ]\n",
    "            \n",
    "            bars2 = ax2.bar(sentiments, percentages, color=colors, alpha=0.8)\n",
    "            ax2.set_title('Pourcentages des sentiments', fontsize=14, fontweight='bold')\n",
    "            ax2.set_ylabel('Pourcentage (%)')\n",
    "            \n",
    "            # Ajouter des étiquettes de pourcentage\n",
    "            for bar, pct in zip(bars2, percentages):\n",
    "                height = bar.get_height()\n",
    "                ax2.text(\n",
    "                    bar.get_x() + bar.get_width()/2., height + max(percentages)*0.01,\n",
    "                    f'{pct:.1f}%', ha='center', va='bottom', fontweight='bold'\n",
    "                )\n",
    "            \n",
    "            # Titre général\n",
    "            fig.suptitle(title, fontsize=16, fontweight='bold', y=1.02)\n",
    "            \n",
    "            # Ajouter des statistiques\n",
    "            stats_text = (\n",
    "                f&quot;Total des publications: {sentiment_summary['total']}\\n&quot;\n",
    "                f&quot;Polarité moyenne: {sentiment_summary.get('average_polarity', 0):.3f}\\n&quot;\n",
    "                f&quot;Confiance moyenne: {sentiment_summary.get('average_confidence', 0):.3f}&quot;\n",
    "            )\n",
    "            \n",
    "            fig.text(0.02, 0.02, stats_text, fontsize=10, \n",
    "                    bbox=dict(boxstyle=&quot;round,pad=0.3&quot;, facecolor=&quot;lightgray&quot;, alpha=0.5))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            return fig\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur lors de la création du graphique à barres des sentiments: {e}&quot;)\n",
    "            return self._create_error_chart(&quot;Graphique à barres des sentiments&quot;)\n",
    "    \n",
    "    def create_keyword_frequency_chart(self, keywords: List[Dict[str, Any]],\n",
    "                                    title: str = &quot;Mots-clés principaux par fréquence&quot;,\n",
    "                                    top_n: int = 20) -> plt.Figure:\n",
    "        &quot;&quot;&quot;Crée un graphique à barres horizontales pour les mots-clés&quot;&quot;&quot;\n",
    "        try:\n",
    "            if not keywords:\n",
    "                return self._create_empty_chart(&quot;Aucun mot-clé disponible&quot;)\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            \n",
    "            # Trier les mots-clés par fréquence\n",
    "            sorted_keywords = sorted(keywords, key=lambda x: x['frequency'], reverse=True)[:top_n]\n",
    "            \n",
    "            # Préparer les données\n",
    "            keyword_names = [kw['keyword'] for kw in reversed(sorted_keywords)]\n",
    "            frequencies = [kw['frequency'] for kw in reversed(sorted_keywords)]\n",
    "            \n",
    "            # Créer un graphique à barres horizontales\n",
    "            bars = ax.barh(keyword_names, frequencies, color=self.colors[0], alpha=0.8)\n",
    "            \n",
    "            # Personnaliser\n",
    "            ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "            ax.set_xlabel('Fréquence')\n",
    "            \n",
    "            # Ajouter des étiquettes de valeur\n",
    "            for bar, freq in zip(bars, frequencies):\n",
    "                width = bar.get_width()\n",
    "                ax.text(\n",
    "                    width + max(frequencies)*0.01, bar.get_y() + bar.get_height()/2.,\n",
    "                    f'{freq}', ha='left', va='center', fontweight='bold'\n",
    "                )\n",
    "            \n",
    "            # Améliorer la mise en page\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            return fig\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur lors de la création du graphique de fréquence des mots-clés: {e}&quot;)\n",
    "            return self._create_error_chart(&quot;Graphique de fréquence des mots-clés&quot;)\n",
    "    \n",
    "    def create_wordcloud(self, keywords: List[Dict[str, Any]], title: str = &quot;Nuage de mots-clés&quot;) -> plt.Figure:\n",
    "        &quot;&quot;&quot;Crée un nuage de mots à partir des mots-clés&quot;&quot;&quot;\n",
    "        try:\n",
    "            if not keywords:\n",
    "                return self._create_empty_chart(&quot;Aucun mot-clé disponible pour le nuage de mots&quot;)\n",
    "            \n",
    "            # Créer un dictionnaire de fréquences pour le nuage de mots\n",
    "            word_freq = {kw['keyword']: kw['frequency'] for kw in keywords}\n",
    "            \n",
    "            # Générer le nuage de mots\n",
    "            wordcloud = WordCloud(\n",
    "                width=800, \n",
    "                height=400, \n",
    "                background_color='white',\n",
    "                max_words=100,\n",
    "                colormap='viridis',\n",
    "                contour_width=1,\n",
    "                contour_color='steelblue'\n",
    "            ).generate_from_frequencies(word_freq)\n",
    "            \n",
    "            # Afficher le nuage de mots\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            ax.imshow(wordcloud, interpolation='bilinear')\n",
    "            ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            return fig\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur lors de la création du nuage de mots: {e}&quot;)\n",
    "            return self._create_error_chart(&quot;Nuage de mots&quot;)\n",
    "    \n",
    "    def _create_error_chart(self, title: str) -> plt.Figure:\n",
    "        &quot;&quot;&quot;Crée un graphique d'erreur lorsque les données ne sont pas disponibles&quot;&quot;&quot;\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.text(0.5, 0.5, 'Erreur lors de la création du graphique\\nDonnées non disponibles', \n",
    "               ha='center', va='center', transform=ax.transAxes,\n",
    "               fontsize=14, color='red')\n",
    "        ax.set_title(f'{title} - Erreur', fontsize=14, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        return fig\n",
    "    \n",
    "    def _create_empty_chart(self, message: str) -> plt.Figure:\n",
    "        &quot;&quot;&quot;Crée un graphique vide avec un message&quot;&quot;&quot;\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.text(0.5, 0.5, message, ha='center', va='center', transform=ax.transAxes,\n",
    "               fontsize=14, color='gray')\n",
    "        ax.axis('off')\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Classe principale d'analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe principale d'analyse\n",
    "class SocialMediaAnalyzer:\n",
    "    &quot;&quot;&quot;Orchestrateur principal pour l'analyse de sentiment des médias sociaux&quot;&quot;&quot;\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.extractor = None\n",
    "        self.sentiment_analyzer = None\n",
    "        self.keyword_extractor = None\n",
    "        self.text_preprocessor = TextPreprocessor()\n",
    "        self.charts_generator = ChartsGenerator()\n",
    "        self.validator = DataValidator()\n",
    "        \n",
    "        logger.info(&quot;SocialMediaAnalyzer initialisé&quot;)\n",
    "    \n",
    "    def analyze(self, service: str, source: str, days: int = 30, max_posts: int = 500,\n",
    "               language: str = 'auto', sentiment_model: str = 'auto',\n",
    "               keyword_method: str = 'combined', progress_callback: Optional[Callable] = None) -> Dict[str, Any]:\n",
    "        &quot;&quot;&quot;\n",
    "        Effectue une analyse complète de sentiment des médias sociaux\n",
    "        \n",
    "        Args:\n",
    "            service: Nom du service/marque à analyser\n",
    "            source: Source de médias sociaux (twitter, facebook, google_reviews)\n",
    "            days: Nombre de jours à analyser\n",
    "            max_posts: Nombre maximum de publications à extraire\n",
    "            language: Langue pour l'analyse\n",
    "            sentiment_model: Modèle d'analyse de sentiment à utiliser\n",
    "            keyword_method: Méthode d'extraction de mots-clés\n",
    "            progress_callback: Fonction de rappel pour les mises à jour de progression\n",
    "        \n",
    "        Returns:\n",
    "            Dictionnaire complet des résultats d'analyse\n",
    "        &quot;&quot;&quot;\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(f&quot;Démarrage de l'analyse pour '{service}' depuis {source}&quot;)\n",
    "            \n",
    "            logger.info(f&quot;Démarrage de l'analyse: service={service}, source={source}, days={days}, max_posts={max_posts}&quot;)\n",
    "            \n",
    "            # Étape 1: Extraction des données\n",
    "            if progress_callback:\n",
    "                progress_callback(&quot;Extraction des données depuis les médias sociaux...&quot;)\n",
    "            \n",
    "            raw_data = self._extract_data(service, source, days, max_posts)\n",
    "            \n",
    "            if not raw_data:\n",
    "                logger.error(&quot;Aucune donnée extraite&quot;)\n",
    "                return {'error': 'Aucune donnée extraite', 'success': False}\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(f&quot;Extrait {len(raw_data)} publications&quot;)\n",
    "            \n",
    "            # Étape 2: Prétraitement du texte\n",
    "            if progress_callback:\n",
    "                progress_callback(&quot;Prétraitement des données textuelles...&quot;)\n",
    "            \n",
    "            processed_data = self._preprocess_data(raw_data, language)\n",
    "            \n",
    "            # Étape 3: Analyse de sentiment\n",
    "            if progress_callback:\n",
    "                progress_callback(&quot;Analyse du sentiment...&quot;)\n",
    "            \n",
    "            sentiment_results = self._analyze_sentiment(processed_data, sentiment_model, language)\n",
    "            \n",
    "            # Étape 4: Extraction de mots-clés\n",
    "            if progress_callback:\n",
    "                progress_callback(&quot;Extraction des mots-clés...&quot;)\n",
    "            \n",
    "            keywords = self._extract_keywords(processed_data, keyword_method)\n",
    "            \n",
    "            # Étape 5: Analyse temporelle (si les données de date sont disponibles)\n",
    "            temporal_analysis = None\n",
    "            if any('created_at' in item for item in processed_data):\n",
    "                if progress_callback:\n",
    "                    progress_callback(&quot;Analyse des tendances temporelles...&quot;)\n",
    "                temporal_analysis = self._analyze_temporal_trends(sentiment_results, processed_data)\n",
    "            \n",
    "            # Étape 6: Générer des statistiques de résumé\n",
    "            if progress_callback:\n",
    "                progress_callback(&quot;Génération des statistiques de résumé...&quot;)\n",
    "            \n",
    "            summary_stats = self._generate_summary_statistics(\n",
    "                sentiment_results, keywords, raw_data, processed_data\n",
    "            )\n",
    "            \n",
    "            # Étape 7: Compiler les résultats\n",
    "            if progress_callback:\n",
    "                progress_callback(&quot;Compilation des résultats finaux...&quot;)\n",
    "            \n",
    "            results = self._compile_results(\n",
    "                service, source, days, max_posts, raw_data, processed_data,\n",
    "                sentiment_results, keywords, temporal_analysis, summary_stats\n",
    "            )\n",
    "            \n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            results['metadata']['execution_time'] = execution_time\n",
    "            \n",
    "            logger.info(f&quot;Analyse terminée avec succès en {execution_time:.2f} secondes&quot;)\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(f&quot;Analyse terminée en {execution_time:.2f} secondes&quot;)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;L'analyse a échoué: {e}&quot;)\n",
    "            return {'error': str(e), 'success': False}\n",
    "    \n",
    "    def _extract_data(self, service: str, source: str, days: int, max_posts: int) -> List[Dict[str, Any]]:\n",
    "        &quot;&quot;&quot;Extrait les données de la source de médias sociaux&quot;&quot;&quot;\n",
    "        try:\n",
    "            # Initialiser l'extracteur approprié\n",
    "            if source.lower() == 'twitter':\n",
    "                self.extractor = TwitterExtractor(service, max_posts)\n",
    "            else:\n",
    "                raise ValueError(f&quot;Source non prise en charge: {source}&quot;)\n",
    "            \n",
    "            # Extraire les données\n",
    "            raw_data = self.extractor.extract_posts(days=days)\n",
    "            \n",
    "            logger.info(f&quot;Extrait {len(raw_data)} publications depuis {source}&quot;)\n",
    "            return raw_data\n",
    "            \n",
    "        except ExtractionError as e:\n",
    "            logger.error(f&quot;L'extraction des données a échoué: {e}&quot;)\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur inattendue lors de l'extraction des données: {e}&quot;)\n",
    "            raise\n",
    "    \n",
    "    def _preprocess_data(self, raw_data: List[Dict[str, Any]], language: str) -> List[Dict[str, Any]]:\n",
    "        &quot;&quot;&quot;Prétraite les données extraites&quot;&quot;&quot;\n",
    "        processed_data = []\n",
    "        \n",
    "        for item in tqdm(raw_data, desc=&quot;Prétraitement des données&quot;):\n",
    "            try:\n",
    "                # Extraire le contenu textuel\n",
    "                text = item.get('text', item.get('message', item.get('content', '')))\n",
    "                \n",
    "                if not text or not self.validator.validate_text_content(text):\n",
    "                    continue\n",
    "                \n",
    "                # Prétraiter le texte\n",
    "                preprocessing_result = self.text_preprocessor.preprocess_text(text, language)\n",
    "                \n",
    "                # Créer un élément prétraité\n",
    "                processed_item = item.copy()\n",
    "                processed_item.update({\n",
    "                    'cleaned_text': preprocessing_result['cleaned'],\n",
    "                    'tokens': preprocessing_result['tokens'],\n",
    "                    'language': preprocessing_result['language'],\n",
    "                    'preprocessing_steps': preprocessing_result['preprocessing_steps']\n",
    "                })\n",
    "                \n",
    "                processed_data.append(processed_item)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f&quot;Erreur lors du prétraitement de l'élément {item.get('id', 'inconnu')}: {e}&quot;)\n",
    "                continue\n",
    "        \n",
    "        logger.info(f&quot;Prétraité {len(processed_data)} éléments&quot;)\n",
    "        return processed_data\n",
    "    \n",
    "    def _analyze_sentiment(self, processed_data: List[Dict[str, Any]], \n",
    "                         sentiment_model: str, language: str) -> List[Dict[str, Any]]:\n",
    "        &quot;&quot;&quot;Analyse le sentiment des données prétraitées&quot;&quot;&quot;\n",
    "        self.sentiment_analyzer = SentimentAnalyzer(sentiment_model, language)\n",
    "        \n",
    "        sentiment_results = []\n",
    "        texts = [item['cleaned_text'] for item in processed_data]\n",
    "        \n",
    "        # Analyser le sentiment pour chaque texte\n",
    "        for i, (item, text) in enumerate(tqdm(zip(processed_data, texts), desc=&quot;Analyse de sentiment&quot;, total=len(texts))):\n",
    "            try:\n",
    "                sentiment_result = self.sentiment_analyzer.analyze_sentiment(\n",
    "                    text, item.get('language', language)\n",
    "                )\n",
    "                \n",
    "                # Ajouter des métadonnées\n",
    "                sentiment_result.update({\n",
    "                    'id': item.get('id'),\n",
    "                    'original_text': item.get('text', ''),\n",
    "                    'date': item.get('created_at'),\n",
    "                    'source': item.get('source'),\n",
    "                    'service': item.get('service')\n",
    "                })\n",
    "                \n",
    "                sentiment_results.append(sentiment_result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f&quot;Erreur lors de l'analyse du sentiment pour l'élément {item.get('id', i)}: {e}&quot;)\n",
    "                continue\n",
    "        \n",
    "        logger.info(f&quot;Sentiment analysé pour {len(sentiment_results)} éléments&quot;)\n",
    "        return sentiment_results\n",
    "    \n",
    "    def _extract_keywords(self, processed_data: List[Dict[str, Any]], \n",
    "                        keyword_method: str) -> List[Dict[str, Any]]:\n",
    "        &quot;&quot;&quot;Extrait les mots-clés des données prétraitées&quot;&quot;&quot;\n",
    "        self.keyword_extractor = KeywordExtractor(\n",
    "            language='auto',  # Utiliser la détection automatique\n",
    "            max_keywords=50\n",
    "        )\n",
    "        \n",
    "        # Extraire les textes pour l'analyse des mots-clés\n",
    "        texts = [item['cleaned_text'] for item in processed_data if item.get('cleaned_text')]\n",
    "        \n",
    "        if not texts:\n",
    "            logger.warning(&quot;Aucun texte disponible pour l'extraction de mots-clés&quot;)\n",
    "            return []\n",
    "        \n",
    "        # Extraire les mots-clés\n",
    "        keywords = self.keyword_extractor.extract_keywords(texts, keyword_method)\n",
    "        \n",
    "        # Extraire également les phrases clés\n",
    "        key_phrases = self.keyword_extractor.extract_key_phrases(texts)\n",
    "        \n",
    "        # Combiner les mots-clés et les phrases\n",
    "        all_keywords = keywords + key_phrases\n",
    "        \n",
    "        # Trier par score et limiter\n",
    "        all_keywords.sort(key=lambda x: x.get('score', 0), reverse=True)\n",
    "        \n",
    "        logger.info(f&quot;Extrait {len(all_keywords)} mots-clés/phrases&quot;)\n",
    "        return all_keywords[:50]  # Limiter aux 50 premiers\n",
    "    \n",
    "    def _analyze_temporal_trends(self, sentiment_results: List[Dict[str, Any]], \n",
    "                               processed_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        &quot;&quot;&quot;Analyse les tendances temporelles du sentiment&quot;&quot;&quot;\n",
    "        try:\n",
    "            # Préparer les données pour l'analyse temporelle\n",
    "            texts = []\n",
    "            dates = []\n",
    "            \n",
    "            for result, item in zip(sentiment_results, processed_data):\n",
    "                if item.get('created_at'):\n",
    "                    texts.append(result.get('original_text', ''))\n",
    "                    dates.append(item['created_at'])\n",
    "            \n",
    "            if not texts or not dates:\n",
    "                logger.warning(&quot;Données temporelles insuffisantes pour l'analyse des tendances&quot;)\n",
    "                return {}\n",
    "            \n",
    "            # Créer un DataFrame pour l'analyse\n",
    "            df = pd.DataFrame({\n",
    "                'date': pd.to_datetime(dates),\n",
    "                'sentiment': [r['sentiment'] for r in sentiment_results if 'sentiment' in r],\n",
    "                'polarity': [r['polarity'] for r in sentiment_results if 'polarity' in r]\n",
    "            })\n",
    "            \n",
    "            # Grouper par jour\n",
    "            df['date'] = df['date'].dt.date\n",
    "            daily = df.groupby('date').agg({\n",
    "                'sentiment': lambda x: x.value_counts().to_dict(),\n",
    "                'polarity': 'mean'\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Convertir en format de résultat\n",
    "            trends = {\n",
    "                'daily_sentiment': daily.to_dict('records'),\n",
    "                'overall_trend': 'stable',  # Simplifié pour cet exemple\n",
    "                'detailed_results': sentiment_results\n",
    "            }\n",
    "            \n",
    "            return trends\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur lors de l'analyse des tendances temporelles: {e}&quot;)\n",
    "            return {}\n",
    "    \n",
    "    def _generate_summary_statistics(self, sentiment_results: List[Dict[str, Any]],\n",
    "                                  keywords: List[Dict[str, Any]], \n",
    "                                  raw_data: List[Dict[str, Any]],\n",
    "                                  processed_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        &quot;&quot;&quot;Génère des statistiques de résumé&quot;&quot;&quot;\n",
    "        try:\n",
    "            # Statistiques de sentiment\n",
    "            sentiment_stats = self.sentiment_analyzer.get_sentiment_summary(sentiment_results)\n",
    "            \n",
    "            # Statistiques d'extraction\n",
    "            extraction_stats = self.extractor.get_extraction_stats() if self.extractor else {}\n",
    "            \n",
    "            # Statistiques de traitement\n",
    "            processing_stats = {\n",
    "                'raw_data_count': len(raw_data),\n",
    "                'processed_data_count': len(processed_data),\n",
    "                'processing_success_rate': (len(processed_data) / len(raw_data) * 100) if raw_data else 0\n",
    "            }\n",
    "            \n",
    "            # Statistiques de mots-clés\n",
    "            keyword_stats = {\n",
    "                'total_keywords': len(keywords),\n",
    "                'avg_keyword_score': sum(kw.get('score', 0) for kw in keywords) / len(keywords) if keywords else 0,\n",
    "                'top_keyword': keywords[0]['keyword'] if keywords else None\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                'sentiment_stats': sentiment_stats,\n",
    "                'extraction_stats': extraction_stats,\n",
    "                'processing_stats': processing_stats,\n",
    "                'keyword_stats': keyword_stats\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur lors de la génération des statistiques de résumé: {e}&quot;)\n",
    "            return {}\n",
    "    \n",
    "    def _compile_results(self, service: str, source: str, days: int, max_posts: int,\n",
    "                       raw_data: List[Dict[str, Any]], processed_data: List[Dict[str, Any]],\n",
    "                       sentiment_results: List[Dict[str, Any]], keywords: List[Dict[str, Any]],\n",
    "                       temporal_analysis: Optional[Dict[str, Any]], \n",
    "                       summary_stats: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        &quot;&quot;&quot;Compile tous les résultats dans une structure finale&quot;&quot;&quot;\n",
    "        try:\n",
    "            results = {\n",
    "                'metadata': {\n",
    "                    'service': service,\n",
    "                    'source': source,\n",
    "                    'analysis_date': datetime.now().isoformat(),\n",
    "                    'parameters': {\n",
    "                        'days': days,\n",
    "                        'max_posts': max_posts,\n",
    "                        'language': 'auto',\n",
    "                        'sentiment_model': 'auto',\n",
    "                        'keyword_method': 'combined'\n",
    "                    }\n",
    "                },\n",
    "                'raw_data': raw_data,\n",
    "                'processed_data': processed_data,\n",
    "                'sentiment_results': sentiment_results,\n",
    "                'sentiment_summary': summary_stats.get('sentiment_stats', {}),\n",
    "                'keywords': keywords,\n",
    "                'temporal_data': temporal_analysis.get('detailed_results', []) if temporal_analysis else [],\n",
    "                'statistics': summary_stats,\n",
    "                'success': True\n",
    "            }\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f&quot;Erreur lors de la compilation des résultats: {e}&quot;)\n",
    "            return {'error': str(e), 'success': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interface utilisateur pour l'analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interface utilisateur pour l'analyse\n",
    "class SentimentAnalysisUI:\n",
    "    &quot;&quot;&quot;Interface utilisateur pour l'analyse de sentiment&quot;&quot;&quot;\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.analyzer = SocialMediaAnalyzer()\n",
    "        self.results = None\n",
    "        self.progress_output = widgets.Output()\n",
    "        self.charts_output = widgets.Output()\n",
    "        self.data_output = widgets.Output()\n",
    "    \n",
    "    def create_ui(self):\n",
    "        &quot;&quot;&quot;Crée l'interface utilisateur&quot;&quot;&quot;\n",
    "        # Paramètres d'entrée\n",
    "        self.service_input = widgets.Text(\n",
    "            value='Apple',\n",
    "            description='Service/Marque:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.source_dropdown = widgets.Dropdown(\n",
    "            options=['twitter'],\n",
    "            value='twitter',\n",
    "            description='Source:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.days_slider = widgets.IntSlider(\n",
    "            value=7,\n",
    "            min=1,\n",
    "            max=30,\n",
    "            step=1,\n",
    "            description='Jours:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.max_posts_slider = widgets.IntSlider(\n",
    "            value=100,\n",
    "            min=10,\n",
    "            max=500,\n",
    "            step=10,\n",
    "            description='Max posts:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.language_dropdown = widgets.Dropdown(\n",
    "            options=['auto', 'english', 'french'],\n",
    "            value='auto',\n",
    "            description='Langue:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.model_dropdown = widgets.Dropdown(\n",
    "            options=['auto', 'textblob', 'transformers'],\n",
    "            value='auto',\n",
    "            description='Modèle:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Bouton d'analyse\n",
    "        self.analyze_button = widgets.Button(\n",
    "            description='Lancer l\\'analyse',\n",
    "            button_style='primary',\n",
    "            icon='play'\n",
    "        )\n",
    "        self.analyze_button.on_click(self._on_analyze_click)\n",
    "        \n",
    "        # Onglets pour les résultats\n",
    "        tabs = widgets.Tab([\n",
    "            self.progress_output,\n",
    "            self.charts_output,\n",
    "            self.data_output\n",
    "        ])\n",
    "        tabs.set_title(0, 'Progression')\n",
    "        tabs.set_title(1, 'Graphiques')\n",
    "        tabs.set_title(2, 'Données')\n",
    "        \n",
    "        # Mise en page\n",
    "        input_widgets = widgets.VBox([\n",
    "            widgets.HBox([self.service_input, self.source_dropdown]),\n",
    "            widgets.HBox([self.days_slider, self.max_posts_slider]),\n",
    "            widgets.HBox([self.language_dropdown, self.model_dropdown]),\n",
    "            self.analyze_button\n",
    "        ])\n",
    "        \n",
    "        # Afficher l'interface\n",
    "        display(widgets.VBox([input_widgets, tabs]))\n",
    "    \n",
    "    def _on_analyze_click(self, b):\n",
    "        &quot;&quot;&quot;Gestionnaire d'événements pour le bouton d'analyse&quot;&quot;&quot;\n",
    "        # Désactiver le bouton pendant l'analyse\n",
    "        self.analyze_button.disabled = True\n",
    "        \n",
    "        # Effacer les sorties précédentes\n",
    "        self.progress_output.clear_output()\n",
    "        self.charts_output.clear_output()\n",
    "        self.data_output.clear_output()\n",
    "        \n",
    "        # Obtenir les paramètres\n",
    "        service = self.service_input.value\n",
    "        source = self.source_dropdown.value\n",
    "        days = self.days_slider.value\n",
    "        max_posts = self.max_posts_slider.value\n",
    "        language = self.language_dropdown.value\n",
    "        model = self.model_dropdown.value\n",
    "        \n",
    "        # Fonction de rappel pour la progression\n",
    "        def progress_callback(message):\n",
    "            with self.progress_output:\n",
    "                print(f&quot;{datetime.now().strftime('%H:%M:%S')} - {message}&quot;)\n",
    "        \n",
    "        # Lancer l'analyse dans un thread séparé\n",
    "        import threading\n",
    "        \n",
    "        def run_analysis():\n",
    "            try:\n",
    "                with self.progress_output:\n",
    "                    print(f&quot;Démarrage de l'analyse pour '{service}' depuis {source}...&quot;)\n",
    "                \n",
    "                # Exécuter l'analyse\n",
    "                self.results = self.analyzer.analyze(\n",
    "                    service=service,\n",
    "                    source=source,\n",
    "                    days=days,\n",
    "                    max_posts=max_posts,\n",
    "                    language=language,\n",
    "                    sentiment_model=model,\n",
    "                    progress_callback=progress_callback\n",
    "                )\n",
    "                \n",
    "                # Afficher les résultats\n",
    "                self._display_results()\n",
    "                \n",
    "            except Exception as e:\n",
    "                with self.progress_output:\n",
    "                    print(f&quot;Erreur lors de l'analyse: {e}&quot;)\n",
    "            finally:\n",
    "                # Réactiver le bouton\n",
    "                self.analyze_button.disabled = False\n",
    "        \n",
    "        # Démarrer l'analyse dans un thread\n",
    "        thread = threading.Thread(target=run_analysis)\n",
    "        thread.start()\n",
    "    \n",
    "    def _display_results(self):\n",
    "        &quot;&quot;&quot;Affiche les résultats de l'analyse&quot;&quot;&quot;\n",
    "        if not self.results or not self.results.get('success', False):\n",
    "            with self.progress_output:\n",
    "                print(&quot;L'analyse a échoué ou n'a pas produit de résultats.&quot;)\n",
    "                if 'error' in self.results:\n",
    "                    print(f&quot;Erreur: {self.results['error']}&quot;)\n",
    "            return\n",
    "        \n",
    "        # Afficher les graphiques\n",
    "        with self.charts_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Créer les graphiques\n",
    "            print(&quot;## Graphiques d'analyse de sentiment&quot;)\n",
    "            \n",
    "            # Graphique circulaire des sentiments\n",
    "            if 'sentiment_summary' in self.results:\n",
    "                fig_pie = self.analyzer.charts_generator.create_sentiment_pie_chart(\n",
    "                    self.results['sentiment_summary'],\n",
    "                    title=f&quot;Distribution des sentiments pour {self.results['metadata']['service']}&quot;\n",
    "                )\n",
    "                plt.show(fig_pie)\n",
    "            \n",
    "            # Graphique à barres des sentiments\n",
    "            if 'sentiment_summary' in self.results:\n",
    "                fig_bar = self.analyzer.charts_generator.create_sentiment_bar_chart(\n",
    "                    self.results['sentiment_summary'],\n",
    "                    title=f&quot;Analyse de sentiment pour {self.results['metadata']['service']}&quot;\n",
    "                )\n",
    "                plt.show(fig_bar)\n",
    "            \n",
    "            # Nuage de mots-clés\n",
    "            if 'keywords' in self.results and self.results['keywords']:\n",
    "                fig_wordcloud = self.analyzer.charts_generator.create_wordcloud(\n",
    "                    self.results['keywords'],\n",
    "                    title=f&quot;Nuage de mots-clés pour {self.results['metadata']['service']}&quot;\n",
    "                )\n",
    "                plt.show(fig_wordcloud)\n",
    "            \n",
    "            # Graphique de fréquence des mots-clés\n",
    "            if 'keywords' in self.results and self.results['keywords']:\n",
    "                fig_keywords = self.analyzer.charts_generator.create_keyword_frequency_chart(\n",
    "                    self.results['keywords'],\n",
    "                    title=f&quot;Mots-clés principaux pour {self.results['metadata']['service']}&quot;,\n",
    "                    top_n=15\n",
    "                )\n",
    "                plt.show(fig_keywords)\n",
    "        \n",
    "        # Afficher les données\n",
    "        with self.data_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            print(&quot;## Résumé de l'analyse&quot;)\n",
    "            print(f&quot;Service analysé: {self.results['metadata']['service']}&quot;)\n",
    "            print(f&quot;Source: {self.results['metadata']['source']}&quot;)\n",
    "            print(f&quot;Date d'analyse: {self.results['metadata']['analysis_date']}&quot;)\n",
    "            print(f&quot;Temps d'exécution: {self.results['metadata']['execution_time']:.2f} secondes&quot;)\n",
    "            print(&quot;\\n&quot;)\n",
    "            \n",
    "            print(&quot;### Statistiques de sentiment&quot;)\n",
    "            sentiment_stats = self.results['sentiment_summary']\n",
    "            print(f&quot;Total des publications: {sentiment_stats['total']}&quot;)\n",
    "            print(f&quot;Positif: {sentiment_stats['positive']} ({sentiment_stats['percentages']['positive']:.1f}%)&quot;)\n",
    "            print(f&quot;Négatif: {sentiment_stats['negative']} ({sentiment_stats['percentages']['negative']:.1f}%)&quot;)\n",
    "            print(f&quot;Neutre: {sentiment_stats['neutral']} ({sentiment_stats['percentages']['neutral']:.1f}%)&quot;)\n",
    "            print(f&quot;Polarité moyenne: {sentiment_stats['average_polarity']:.3f}&quot;)\n",
    "            print(&quot;\\n&quot;)\n",
    "            \n",
    "            print(&quot;### Mots-clés principaux&quot;)\n",
    "            if 'keywords' in self.results and self.results['keywords']:\n",
    "                top_keywords = self.results['keywords'][:10]\n",
    "                for i, kw in enumerate(top_keywords, 1):\n",
    "                    print(f&quot;{i}. {kw['keyword']} (score: {kw['score']:.3f}, fréquence: {kw.get('frequency', 'N/A')})&quot;)\n",
    "            else:\n",
    "                print(&quot;Aucun mot-clé extrait&quot;)\n",
    "            print(&quot;\\n&quot;)\n",
    "            \n",
    "            print(&quot;### Exemples de publications&quot;)\n",
    "            if 'sentiment_results' in self.results and self.results['sentiment_results']:\n",
    "                # Afficher quelques exemples de chaque catégorie\n",
    "                sentiment_results = self.results['sentiment_results']\n",
    "                \n",
    "                # Positif\n",
    "                positive = [r for r in sentiment_results if r['sentiment'] == 'positive'][:3]\n",
    "                if positive:\n",
    "                    print(&quot;\\n**Publications positives:**&quot;)\n",
    "                    for i, p in enumerate(positive, 1):\n",
    "                        print(f&quot;{i}. \\&quot;{p['original_text'][:100]}...\\&quot; (polarité: {p['polarity']:.2f})&quot;)\n",
    "                \n",
    "                # Négatif\n",
    "                negative = [r for r in sentiment_results if r['sentiment'] == 'negative'][:3]\n",
    "                if negative:\n",
    "                    print(&quot;\\n**Publications négatives:**&quot;)\n",
    "                    for i, n in enumerate(negative, 1):\n",
    "                        print(f&quot;{i}. \\&quot;{n['original_text'][:100]}...\\&quot; (polarité: {n['polarity']:.2f})&quot;)\n",
    "                \n",
    "                # Neutre\n",
    "                neutral = [r for r in sentiment_results if r['sentiment'] == 'neutral'][:3]\n",
    "                if neutral:\n",
    "                    print(&quot;\\n**Publications neutres:**&quot;)\n",
    "                    for i, n in enumerate(neutral, 1):\n",
    "                        print(f&quot;{i}. \\&quot;{n['original_text'][:100]}...\\&quot; (polarité: {n['polarity']:.2f})&quot;)\n",
    "            else:\n",
    "                print(&quot;Aucun résultat d'analyse de sentiment disponible&quot;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Lancement de l'interface utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer et afficher l'interface utilisateur\n",
    "ui = SentimentAnalysisUI()\n",
    "ui.create_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exemple d'analyse manuelle (alternative à l'interface utilisateur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation manuelle (sans interface utilisateur)\n",
    "def run_manual_analysis():\n",
    "    &quot;&quot;&quot;Exécute une analyse manuelle sans l'interface utilisateur&quot;&quot;&quot;\n",
    "    # Paramètres d'analyse\n",
    "    service = &quot;Apple&quot;  # Remplacez par le service/marque que vous souhaitez analyser\n",
    "    source = &quot;twitter&quot;\n",
    "    days = 7\n",
    "    max_posts = 100\n",
    "    \n",
    "    # Fonction de rappel pour la progression\n",
    "    def progress_callback(message):\n",
    "        print(f&quot;{datetime.now().strftime('%H:%M:%S')} - {message}&quot;)\n",
    "    \n",
    "    # Créer l'analyseur et exécuter l'analyse\n",
    "    analyzer = SocialMediaAnalyzer()\n",
    "    results = analyzer.analyze(\n",
    "        service=service,\n",
    "        source=source,\n",
    "        days=days,\n",
    "        max_posts=max_posts,\n",
    "        progress_callback=progress_callback\n",
    "    )\n",
    "    \n",
    "    # Vérifier si l'analyse a réussi\n",
    "    if not results or not results.get('success', False):\n",
    "        print(&quot;L'analyse a échoué ou n'a pas produit de résultats.&quot;)\n",
    "        if 'error' in results:\n",
    "            print(f&quot;Erreur: {results['error']}&quot;)\n",
    "        return\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    print(&quot;\\n## Résumé de l'analyse&quot;)\n",
    "    print(f&quot;Service analysé: {results['metadata']['service']}&quot;)\n",
    "    print(f&quot;Source: {results['metadata']['source']}&quot;)\n",
    "    print(f&quot;Date d'analyse: {results['metadata']['analysis_date']}&quot;)\n",
    "    print(f&quot;Temps d'exécution: {results['metadata']['execution_time']:.2f} secondes&quot;)\n",
    "    \n",
    "    # Afficher les statistiques de sentiment\n",
    "    sentiment_stats = results['sentiment_summary']\n",
    "    print(&quot;\\n### Statistiques de sentiment&quot;)\n",
    "    print(f&quot;Total des publications: {sentiment_stats['total']}&quot;)\n",
    "    print(f&quot;Positif: {sentiment_stats['positive']} ({sentiment_stats['percentages']['positive']:.1f}%)&quot;)\n",
    "    print(f&quot;Négatif: {sentiment_stats['negative']} ({sentiment_stats['percentages']['negative']:.1f}%)&quot;)\n",
    "    print(f&quot;Neutre: {sentiment_stats['neutral']} ({sentiment_stats['percentages']['neutral']:.1f}%)&quot;)\n",
    "    \n",
    "    # Créer et afficher les graphiques\n",
    "    charts_generator = ChartsGenerator()\n",
    "    \n",
    "    # Graphique circulaire des sentiments\n",
    "    fig_pie = charts_generator.create_sentiment_pie_chart(\n",
    "        results['sentiment_summary'],\n",
    "        title=f&quot;Distribution des sentiments pour {results['metadata']['service']}&quot;\n",
    "    )\n",
    "    plt.show(fig_pie)\n",
    "    \n",
    "    # Nuage de mots-clés\n",
    "    if 'keywords' in results and results['keywords']:\n",
    "        fig_wordcloud = charts_generator.create_wordcloud(\n",
    "            results['keywords'],\n",
    "            title=f&quot;Nuage de mots-clés pour {results['metadata']['service']}&quot;\n",
    "        )\n",
    "        plt.show(fig_wordcloud)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Décommentez la ligne suivante pour exécuter l'analyse manuelle\n",
    "# results = run_manual_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Documentation et limitations\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Extraction de données**:\n",
    "   - Cette version utilise uniquement snscrape pour Twitter, qui peut être limité par les politiques de Twitter\n",
    "   - Facebook et Google Reviews ne sont pas implémentés dans cette version Colab\n",
    "\n",
    "2. **Analyse de sentiment**:\n",
    "   - TextBlob offre une analyse de sentiment basique qui peut manquer de précision pour certains contextes\n",
    "   - Le modèle Transformers nécessite un téléchargement initial qui peut prendre du temps\n",
    "\n",
    "3. **Performance**:\n",
    "   - Les ressources de Colab peuvent limiter la taille des analyses\n",
    "   - L'extraction d'un grand nombre de posts peut être lente\n",
    "\n",
    "### Conseils d'utilisation\n",
    "\n",
    "1. Commencez avec un petit nombre de publications (50-100) pour tester\n",
    "2. Pour des analyses plus rapides, utilisez TextBlob au lieu de Transformers\n",
    "3. Sauvegardez les résultats importants car les sessions Colab peuvent expirer\n",
    "4. Pour des analyses plus précises en français, utilisez le modèle Transformers\n",
    "\n",
    "### Extensions possibles\n",
    "\n",
    "1. Ajouter d'autres sources de médias sociaux\n",
    "2. Implémenter une analyse de sentiment plus avancée\n",
    "3. Ajouter des fonctionnalités d'exportation des résultats\n",
    "4. Intégrer une analyse d'émotions plus détaillée\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}